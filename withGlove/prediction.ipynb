{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import tempfile\n",
    "import sys\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum\n",
    "from unidecode import unidecode\n",
    "import pickle\n",
    "import spacy\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "# sp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def strip_custom(token):\n",
    "    token = token.replace('&Reg;', \" \")\n",
    "    token = token.replace(\"&lt;\", \"<\")\n",
    "    token = token.replace(\"&times;\", \"\")\n",
    "    token = token.replace(\"&gt;\", \">\")\n",
    "    token = token.replace(\"&quot;\", \"\")\n",
    "    token = token.replace('&nbsp', \" \")\n",
    "    token = token.replace('&copy;', \" \")\n",
    "    token = token.replace('&reg', \" \")\n",
    "    token = token.replace('%20', \" \")\n",
    "    # this has to be last:\n",
    "    token = token.replace(\"&amp;\", \"&\")\n",
    "    token = token.replace(\"â\\x80¢\", \" \")\n",
    "#     token = token.replace(\"Â®\", \" \")\n",
    "#     token = token.replace(\"Ã©\", \" \")\n",
    "#     token = token.replace(\"®\", \" \")\n",
    "#     token = token.replace(\"©\", \" \")\n",
    "#     token = token.replace(\"™\", \" \")\n",
    "#     token = token.replace(\"•\", \"\")\n",
    "#     token = token.replace(\"�\", \"\")\n",
    "    \n",
    "    token = token.replace(\"width:99pt\", \"\")\n",
    "    token = token.replace('class=\"xl66\">', '')\n",
    "    token = token.replace('&#160;', ' ')\n",
    "    return token\n",
    "\n",
    "\n",
    "def string_processor(token):\n",
    "    str = unidecode(token)\n",
    "    str = str.lower()\n",
    "    \n",
    "    str = strip_custom(str)\n",
    "    str = remove_stopwords(str)\n",
    "    str = remove_tags(str)\n",
    "    str = strip_punctuation(str)\n",
    "    str = strip_non_alphanum(str)\n",
    "    str = strip_multiple_whitespaces(str)\n",
    "\n",
    "    return str\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saved_token_and_Tosentence(corpus,savedToken):\n",
    "#     corpus = df.description.tolist()\n",
    "\n",
    "\n",
    "    # loading\n",
    "    with open(savedToken, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "#     tokenizer.fit_on_texts(corpus)\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    top_word = len(tokenizer.index_word) +1\n",
    "    print(\"vocab:\",top_word)\n",
    "    return np.array(sequences)\n",
    "\n",
    "def processing_sequences(x_test,maxlen=16):\n",
    "    print(len(x_test), 'sample sequences')\n",
    "\n",
    "    print('Average sample sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_test)))))\n",
    "    print('Pad sequences (samples x time)')\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "    print('X shape:', x_test.shape)\n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop:  20665\n",
      "after drop:  11066\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('newtest.csv')\n",
    "\n",
    "print(\"before drop: \",len(df))\n",
    "df = df.dropna()\n",
    "print(\"after drop: \",len(df))\n",
    "\n",
    "df = df[['bucket_name','description','product_name']]\n",
    "# to lowercase\n",
    "df.bucket_name = df.bucket_name.apply(lambda x : x.lower())\n",
    "df.description = df.description.apply(lambda x : string_processor(x))\n",
    "df.product_name = df.product_name.apply(lambda x :string_processor(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load('namesaved_encoder.npy',allow_pickle=True)\n",
    "label = encoder.transform(df['bucket_name'])\n",
    "N=len(encoder.classes_)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 17305\n",
      "vocab: 101235\n",
      "11066 sample sequences\n",
      "Average sample sequence length: 5.683625519609615\n",
      "Pad sequences (samples x time)\n",
      "X shape: (11066, 16)\n",
      "11066 sample sequences\n",
      "Average sample sequence length: 63.47026929333092\n",
      "Pad sequences (samples x time)\n",
      "X shape: (11066, 75)\n"
     ]
    }
   ],
   "source": [
    "name_token = get_saved_token_and_Tosentence(df.product_name.tolist(),'nametokenizer.pickle')\n",
    "des_token = get_saved_token_and_Tosentence(df.description.tolist(),'descriptiontokenizer.pickle')\n",
    "\n",
    "X_name = processing_sequences(name_token,maxlen=16)\n",
    "X_des = processing_sequences(des_token,maxlen=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "def predict_json(project, model, instances, version=None):\n",
    "    \"\"\"Send json data to a deployed model for prediction.\n",
    "\n",
    "    Args:\n",
    "        project (str): project where the Cloud ML Engine Model is deployed.\n",
    "        model (str): model name.\n",
    "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
    "            your deployed model expects as inputs. Values should be datatypes\n",
    "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
    "            convertible to tensors.\n",
    "        version: str, version of the model to target.\n",
    "    Returns:\n",
    "        Mapping[str: any]: dictionary of prediction results defined by the\n",
    "            model.\n",
    "    \"\"\"\n",
    "    # Create the ML Engine service object.\n",
    "    # To authenticate set the environment variable\n",
    "    # GOOGLE_APPLICATION_CREDENTIALS=<path_to_service_account_file>\n",
    "    service = googleapiclient.discovery.build('ml', 'v1')\n",
    "    name = 'projects/{}/models/{}'.format(project, model)\n",
    "\n",
    "    if version is not None:\n",
    "        name += '/versions/{}'.format(version)\n",
    "\n",
    "    response = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': instances}\n",
    "    ).execute()\n",
    "\n",
    "    if 'error' in response:\n",
    "        raise RuntimeError(response['error'])\n",
    "\n",
    "    return response['predictions']\n",
    "\n",
    "\n",
    "# def get_true_label(encoder,y_hats):\n",
    "#     labels = [np.argmax(np.array(i['dense'])) for i in y_hats]\n",
    "#     return encoder.inverse_transform(labels),labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(y_hat):\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_prediction(project, model, instances):\n",
    "    instances = instances.tolist()\n",
    "    y_hat = predict_json(project,model,instances)\n",
    "    return np.array([np.array(i['dense']) for i in y_hat ])\n",
    "\n",
    "def get_labels(project, model,batch_size,X):\n",
    "\n",
    "    result_probs = []\n",
    "    batches = list(range(0,len(X),batch_size))+[len(X)]\n",
    "    for i in range(len(batches)-1):\n",
    "        print('processing...',batches[i])\n",
    "        probs = get_batch_prediction(project, model,X[batches[i]:batches[i+1]])\n",
    "        result_probs.extend(probs)\n",
    "    return np.array(result_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing... 0\n",
      "processing... 10000\n",
      "CPU times: user 652 ms, sys: 72 ms, total: 724 ms\n",
      "Wall time: 7.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_hat_name = get_labels('groupby-development','FastTextNameGlove',10000,X_name)\n",
    "y_hat_name_label =np.argmax(y_hat_name ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing... 0\n",
      "processing... 2000\n",
      "processing... 4000\n",
      "processing... 6000\n",
      "processing... 8000\n",
      "processing... 10000\n",
      "CPU times: user 904 ms, sys: 28 ms, total: 932 ms\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_hat_des = get_labels('groupby-development','FastTextDesGlove',2000,X_des)\n",
    "y_hat_des_label =np.argmax(y_hat_des ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9524    0.5263    0.6780        38\n",
      "           2     0.8000    0.8000    0.8000        30\n",
      "           3     0.1176    0.6667    0.2000         3\n",
      "           4     0.8780    0.9863    0.9290       146\n",
      "           5     0.7432    0.8209    0.7801       134\n",
      "           6     0.5567    0.7248    0.6297       149\n",
      "           7     0.0000    0.0000    0.0000        40\n",
      "           8     0.5500    0.5500    0.5500        20\n",
      "           9     0.6741    0.7982    0.7309       228\n",
      "          10     0.9000    0.8182    0.8571        11\n",
      "          13     0.8712    0.7808    0.8235       762\n",
      "          14     0.8280    0.8442    0.8360       154\n",
      "          15     0.9043    0.9130    0.9087       207\n",
      "          16     0.9147    0.8881    0.9012       447\n",
      "          17     0.6757    0.7426    0.7075       101\n",
      "          18     0.9057    0.8303    0.8664       613\n",
      "          19     0.2000    1.0000    0.3333         1\n",
      "          20     0.0000    0.0000    0.0000         2\n",
      "          21     0.8935    0.8042    0.8465       991\n",
      "          22     0.3514    0.9286    0.5098        28\n",
      "          24     0.8499    0.8814    0.8654       784\n",
      "          25     0.3538    1.0000    0.5227        23\n",
      "          26     0.6226    0.7857    0.6947        42\n",
      "          27     0.8864    0.7678    0.8228       366\n",
      "          28     0.6737    0.8649    0.7574        74\n",
      "          29     0.0000    0.0000    0.0000         0\n",
      "          30     0.3750    0.7500    0.5000         4\n",
      "          31     0.0000    0.0000    0.0000         5\n",
      "          32     0.0132    1.0000    0.0260         1\n",
      "          33     0.0000    0.0000    0.0000         1\n",
      "          35     0.8796    0.6738    0.7631       141\n",
      "          36     0.9899    0.9800    0.9849       100\n",
      "          37     0.9220    0.9813    0.9507       747\n",
      "          38     0.7562    0.6090    0.6747      1243\n",
      "          39     0.3333    0.0968    0.1500        62\n",
      "          40     0.9211    0.7825    0.8462       492\n",
      "          41     0.9714    0.8831    0.9252        77\n",
      "          42     0.0000    0.0000    0.0000         2\n",
      "          43     1.0000    0.5000    0.6667         8\n",
      "          44     0.0526    0.5000    0.0952         2\n",
      "          45     0.8000    0.4444    0.5714         9\n",
      "          46     0.0000    0.0000    0.0000         0\n",
      "          47     0.9277    0.8370    0.8800        92\n",
      "          48     0.0000    0.0000    0.0000         1\n",
      "          49     0.3333    0.0476    0.0833        42\n",
      "          50     0.0000    0.0000    0.0000         0\n",
      "          52     0.5420    0.8161    0.6514        87\n",
      "          53     0.6864    0.6815    0.6839      1262\n",
      "          54     0.8036    0.4455    0.5732       101\n",
      "          55     0.0000    0.0000    0.0000         0\n",
      "          56     0.7875    0.8873    0.8344        71\n",
      "          57     0.6901    0.6484    0.6686       182\n",
      "          58     0.6346    0.3952    0.4871       167\n",
      "          59     0.6471    0.2355    0.3453       327\n",
      "          60     0.1429    0.1818    0.1600        11\n",
      "          61     0.7762    0.8301    0.8022       259\n",
      "          64     0.8889    0.5269    0.6617       167\n",
      "          65     0.0105    0.7778    0.0207         9\n",
      "\n",
      "    accuracy                         0.7471     11066\n",
      "   macro avg     0.5515    0.5903    0.5268     11066\n",
      "weighted avg     0.8032    0.7471    0.7657     11066\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6552    0.5000    0.5672        38\n",
      "           2     0.3448    0.3333    0.3390        30\n",
      "           3     0.2000    0.3333    0.2500         3\n",
      "           4     0.7308    0.9110    0.8110       146\n",
      "           5     0.7068    0.7015    0.7041       134\n",
      "           6     0.7877    0.7718    0.7797       149\n",
      "           7     0.1333    0.0500    0.0727        40\n",
      "           8     0.6667    0.1000    0.1739        20\n",
      "           9     0.8052    0.8158    0.8105       228\n",
      "          10     0.9000    0.8182    0.8571        11\n",
      "          11     0.0000    0.0000    0.0000         0\n",
      "          13     0.7591    0.7402    0.7495       762\n",
      "          14     0.8675    0.9351    0.9000       154\n",
      "          15     0.8930    0.9275    0.9100       207\n",
      "          16     0.9258    0.7539    0.8311       447\n",
      "          17     0.6962    0.5446    0.6111       101\n",
      "          18     0.8211    0.8238    0.8225       613\n",
      "          19     0.3333    1.0000    0.5000         1\n",
      "          20     0.0000    0.0000    0.0000         2\n",
      "          21     0.8318    0.8385    0.8352       991\n",
      "          22     0.9167    0.7857    0.8462        28\n",
      "          24     0.8593    0.9426    0.8990       784\n",
      "          25     0.5909    0.5652    0.5778        23\n",
      "          26     0.6809    0.7619    0.7191        42\n",
      "          27     0.8991    0.7787    0.8346       366\n",
      "          28     0.9342    0.9595    0.9467        74\n",
      "          29     0.0000    0.0000    0.0000         0\n",
      "          30     1.0000    0.2500    0.4000         4\n",
      "          31     0.0000    0.0000    0.0000         5\n",
      "          32     0.0625    1.0000    0.1176         1\n",
      "          33     0.0000    0.0000    0.0000         1\n",
      "          34     0.0000    0.0000    0.0000         0\n",
      "          35     0.5859    0.8227    0.6844       141\n",
      "          36     0.8333    0.8000    0.8163       100\n",
      "          37     0.8693    0.8635    0.8664       747\n",
      "          38     0.6370    0.4714    0.5418      1243\n",
      "          39     0.6154    0.2581    0.3636        62\n",
      "          40     0.9357    0.8577    0.8950       492\n",
      "          41     0.8974    0.9091    0.9032        77\n",
      "          42     0.5000    0.5000    0.5000         2\n",
      "          43     0.8889    1.0000    0.9412         8\n",
      "          44     1.0000    1.0000    1.0000         2\n",
      "          45     1.0000    0.5556    0.7143         9\n",
      "          47     0.9881    0.9022    0.9432        92\n",
      "          48     0.1429    1.0000    0.2500         1\n",
      "          49     0.7143    0.2381    0.3571        42\n",
      "          52     0.5632    0.5632    0.5632        87\n",
      "          53     0.5775    0.8177    0.6769      1262\n",
      "          54     0.5164    0.6238    0.5650       101\n",
      "          55     0.0000    0.0000    0.0000         0\n",
      "          56     0.9692    0.8873    0.9265        71\n",
      "          57     0.7814    0.7857    0.7836       182\n",
      "          58     0.4494    0.4251    0.4369       167\n",
      "          59     0.5808    0.3517    0.4381       327\n",
      "          60     0.6667    0.3636    0.4706        11\n",
      "          61     0.7218    0.6911    0.7061       259\n",
      "          64     0.6395    0.5629    0.5987       167\n",
      "          65     0.0682    0.6667    0.1237         9\n",
      "\n",
      "    accuracy                         0.7435     11066\n",
      "   macro avg     0.6059    0.6010    0.5678     11066\n",
      "weighted avg     0.7538    0.7435    0.7409     11066\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8750    0.5526    0.6774        38\n",
      "           2     0.6486    0.8000    0.7164        30\n",
      "           3     0.2000    0.6667    0.3077         3\n",
      "           4     0.8430    0.9932    0.9119       146\n",
      "           5     0.7931    0.8582    0.8244       134\n",
      "           6     0.7593    0.8255    0.7910       149\n",
      "           7     0.0000    0.0000    0.0000        40\n",
      "           8     0.7273    0.4000    0.5161        20\n",
      "           9     0.7829    0.8860    0.8313       228\n",
      "          10     1.0000    0.9091    0.9524        11\n",
      "          11     0.0000    0.0000    0.0000         0\n",
      "          13     0.8672    0.8399    0.8533       762\n",
      "          14     0.8588    0.9870    0.9184       154\n",
      "          15     0.9238    0.9952    0.9581       207\n",
      "          16     0.9809    0.9172    0.9480       447\n",
      "          17     0.7245    0.7030    0.7136       101\n",
      "          18     0.8932    0.9005    0.8968       613\n",
      "          19     0.3333    1.0000    0.5000         1\n",
      "          20     0.0000    0.0000    0.0000         2\n",
      "          21     0.8908    0.8809    0.8858       991\n",
      "          22     0.6970    0.8214    0.7541        28\n",
      "          24     0.8663    0.9503    0.9063       784\n",
      "          25     0.6471    0.9565    0.7719        23\n",
      "          26     0.6667    0.7619    0.7111        42\n",
      "          27     0.9207    0.8251    0.8703       366\n",
      "          28     0.9722    0.9459    0.9589        74\n",
      "          29     0.0000    0.0000    0.0000         0\n",
      "          30     1.0000    0.7500    0.8571         4\n",
      "          31     0.0000    0.0000    0.0000         5\n",
      "          32     0.0435    1.0000    0.0833         1\n",
      "          33     0.0000    0.0000    0.0000         1\n",
      "          34     0.0000    0.0000    0.0000         0\n",
      "          35     0.7485    0.8652    0.8026       141\n",
      "          36     1.0000    0.9800    0.9899       100\n",
      "          37     0.9379    0.9505    0.9441       747\n",
      "          38     0.7738    0.5696    0.6562      1243\n",
      "          39     0.7500    0.1935    0.3077        62\n",
      "          40     0.9506    0.9004    0.9248       492\n",
      "          41     0.9589    0.9091    0.9333        77\n",
      "          42     0.0000    0.0000    0.0000         2\n",
      "          43     0.8889    1.0000    0.9412         8\n",
      "          44     1.0000    1.0000    1.0000         2\n",
      "          45     1.0000    0.5556    0.7143         9\n",
      "          47     0.9670    0.9565    0.9617        92\n",
      "          48     0.0000    0.0000    0.0000         1\n",
      "          49     0.7143    0.2381    0.3571        42\n",
      "          52     0.8148    0.7586    0.7857        87\n",
      "          53     0.6529    0.8273    0.7298      1262\n",
      "          54     0.6667    0.6337    0.6497       101\n",
      "          56     0.8889    0.9014    0.8951        71\n",
      "          57     0.8343    0.8022    0.8179       182\n",
      "          58     0.6377    0.5269    0.5770       167\n",
      "          59     0.7256    0.3639    0.4847       327\n",
      "          60     1.0000    0.1818    0.3077        11\n",
      "          61     0.7596    0.8417    0.7985       259\n",
      "          64     0.8045    0.6407    0.7133       167\n",
      "          65     0.0381    0.8889    0.0731         9\n",
      "\n",
      "    accuracy                         0.8092     11066\n",
      "   macro avg     0.6566    0.6599    0.6225     11066\n",
      "weighted avg     0.8257    0.8092    0.8094     11066\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(label, y_hat_name_label,digits=4))\n",
    "print(classification_report(label, y_hat_des_label,digits=4))\n",
    "print(classification_report(label, np.argmax(y_hat_des+y_hat_name ,axis=1),digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
