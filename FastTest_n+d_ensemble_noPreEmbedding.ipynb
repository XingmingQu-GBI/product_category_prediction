{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  1.15.0\n",
      "Eager mode:  False\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling1D,Embedding\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bucket_name</th>\n",
       "      <th>description</th>\n",
       "      <th>product_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>food decorating tools &amp; presentation accessories</td>\n",
       "      <td>don't keep your guests guessing at your next d...</td>\n",
       "      <td>chalkboard gift tags black pkg/5  3\" x 1-1/2\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>food decorating tools &amp; presentation accessories</td>\n",
       "      <td>don't keep your guests guessing at your next d...</td>\n",
       "      <td>ingredient spring stainless steel pkg/4  1-1/4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food decorating tools &amp; presentation accessories</td>\n",
       "      <td>don't keep your guests guessing at your next d...</td>\n",
       "      <td>chalk markers chisel tip classic pkg/8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>food decorating tools &amp; presentation accessories</td>\n",
       "      <td>add a personalized touch to party tables with ...</td>\n",
       "      <td>fine tip chalk markers white pkg/2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>food decorating tools &amp; presentation accessories</td>\n",
       "      <td>add a personalized touch to party tables with ...</td>\n",
       "      <td>fine tip chalk markers white pkg/2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        bucket_name  \\\n",
       "0  food decorating tools & presentation accessories   \n",
       "1  food decorating tools & presentation accessories   \n",
       "2  food decorating tools & presentation accessories   \n",
       "3  food decorating tools & presentation accessories   \n",
       "4  food decorating tools & presentation accessories   \n",
       "\n",
       "                                         description  \\\n",
       "0  don't keep your guests guessing at your next d...   \n",
       "1  don't keep your guests guessing at your next d...   \n",
       "2  don't keep your guests guessing at your next d...   \n",
       "3  add a personalized touch to party tables with ...   \n",
       "4  add a personalized touch to party tables with ...   \n",
       "\n",
       "                                        product_name  \n",
       "0      chalkboard gift tags black pkg/5  3\" x 1-1/2\"  \n",
       "1  ingredient spring stainless steel pkg/4  1-1/4...  \n",
       "2             chalk markers chisel tip classic pkg/8  \n",
       "3                 fine tip chalk markers white pkg/2  \n",
       "4                 fine tip chalk markers white pkg/2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../test_30k.csv')\n",
    "df = df[['bucket_name','description','product_name']]\n",
    "# to lowercase\n",
    "df.bucket_name = df.bucket_name.apply(lambda x : x.lower())\n",
    "df.description = df.description.apply(lambda x : str(x).lower())\n",
    "df.product_name = df.product_name.apply(lambda x : str(x).lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glasses                                                               4335\n",
       "tableware variety packs                                               3501\n",
       "plates & platters                                                     2758\n",
       "tablecloths & table runners                                           1959\n",
       "bowls                                                                 1943\n",
       "food storage canisters/containers & accessories                       1531\n",
       "mugs/teacups/cups                                                     1233\n",
       "napkins                                                               1138\n",
       "coolers & food carriers                                               1108\n",
       "placemats                                                             1071\n",
       "trays & boards                                                         866\n",
       "kitchen/dishtowels & dishcloths                                        791\n",
       "vacuum/water bottles & travel mugs                                     781\n",
       "flatware sets                                                          597\n",
       "knives                                                                 476\n",
       "coffee, tea, & espresso makers                                         413\n",
       "kitchen textiles & clothing replacement parts & accessories            383\n",
       "frying pans & skillets                                                 350\n",
       "baking dishes                                                          345\n",
       "coasters                                                               341\n",
       "chopping/slicing/cutting boards                                        304\n",
       "kitchen merchandise variety packs                                      289\n",
       "pitchers & carafes                                                     282\n",
       "dutch ovens & braisers                                                 251\n",
       "ice buckets & wine bottle chillers                                     243\n",
       "openers                                                                234\n",
       "teapots & teakettles                                                   223\n",
       "seasoning shakers & mills                                              220\n",
       "aprons                                                                 219\n",
       "cookware & bakeware variety packs                                      216\n",
       "                                                                      ... \n",
       "saucepans                                                              128\n",
       "food preparation/mixing bowls                                          117\n",
       "peelers, slicers, & graters                                            116\n",
       "food & beverage preparation device accessories & replacement parts     112\n",
       "bottle stoppers & pourers                                              111\n",
       "serving utensils                                                       110\n",
       "dish drying racks & mats                                               109\n",
       "grinders, juicers, & ice crushers                                      109\n",
       "trivets                                                                106\n",
       "coffee, tea, & espresso preparation tools                              102\n",
       "utensil holders/crocks                                                 102\n",
       "cruets & sprayers                                                      101\n",
       "dough & baking tools                                                    87\n",
       "food storage baskets                                                    85\n",
       "steamers, pasta & stock pots                                            84\n",
       "food processors                                                         83\n",
       "knife blocks, racks, & rolls                                            81\n",
       "forks                                                                   80\n",
       "spice racks                                                             77\n",
       "serving utensils & cutlery variety packs                                76\n",
       "disposable food storage                                                 76\n",
       "food decorating tools & presentation accessories                        74\n",
       "measuring cups                                                          74\n",
       "cookware & bakeware replacement parts & accessories                     71\n",
       "slow & pressure cookers                                                 69\n",
       "colanders & strainers                                                   68\n",
       "spoon rests                                                             67\n",
       "turntables/lazy susan trays                                             64\n",
       "reusable food storage bags                                              59\n",
       "tableware storage & accessories                                         56\n",
       "Name: bucket_name, Length: 77, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bucket_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories: 77\n"
     ]
    }
   ],
   "source": [
    "print(\"categories:\",len(df.bucket_name.unique()))\n",
    "CLASSES = LabelEncoder()\n",
    "df['label']=CLASSES.fit_transform(df['bucket_name'])\n",
    "df.head()\n",
    "NUM_OF_CLASS=len(CLASSES.classes_)\n",
    "label = df.label.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aprons', 'baking dishes', 'baking sheets & baking pans',\n",
       "       'blenders', 'bottle stoppers & pourers', 'bowls',\n",
       "       'chopping/slicing/cutting boards', 'coasters',\n",
       "       'cocktail preparation tools', 'coffee, tea, & espresso makers',\n",
       "       'coffee, tea, & espresso preparation tools',\n",
       "       'colanders & strainers', 'cooking utensils variety packs',\n",
       "       'cooking/baking spoons & spatulas',\n",
       "       'cookware & bakeware replacement parts & accessories',\n",
       "       'cookware & bakeware storage racks',\n",
       "       'cookware & bakeware variety packs', 'coolers & food carriers',\n",
       "       'cruets & sprayers', 'decanters', 'dish drying racks & mats',\n",
       "       'dispensers', 'disposable food storage', 'dough & baking tools',\n",
       "       'dutch ovens & braisers', 'flatware sets',\n",
       "       'food & beverage preparation device accessories & replacement parts',\n",
       "       'food decorating tools & presentation accessories',\n",
       "       'food preparation/mixing bowls', 'food processors',\n",
       "       'food storage baskets',\n",
       "       'food storage canisters/containers & accessories', 'forks',\n",
       "       'frying pans & skillets', 'glasses', 'grill pans & griddles',\n",
       "       'grinders, juicers, & ice crushers',\n",
       "       'ice buckets & wine bottle chillers',\n",
       "       'kitchen merchandise variety packs',\n",
       "       'kitchen textiles & clothing replacement parts & accessories',\n",
       "       'kitchen/dishtowels & dishcloths', 'knife blocks, racks, & rolls',\n",
       "       'knives', 'measuring cups', 'mixers', 'mugs/teacups/cups',\n",
       "       'napkins', 'openers', 'peelers, slicers, & graters',\n",
       "       'pitchers & carafes', 'placemats', 'plates & platters',\n",
       "       'potholders & oven mitts', 'reusable food storage bags',\n",
       "       'saucepans', 'seasoning shakers & mills', 'serving utensils',\n",
       "       'serving utensils & cutlery variety packs', 'shaker bottles',\n",
       "       'slow & pressure cookers', 'specialty serving dishes',\n",
       "       'spice racks', 'spoon rests', 'spoons', 'stands',\n",
       "       'steamers, pasta & stock pots', 'tablecloths & table runners',\n",
       "       'tableware storage & accessories', 'tableware variety packs',\n",
       "       'teapots & teakettles', 'toasters & toaster ovens',\n",
       "       'trays & boards', 'trivets', 'turntables/lazy susan trays',\n",
       "       'utensil holders/crocks', 'vacuum/water bottles & travel mugs',\n",
       "       'wine racks & bottle holders'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASSES.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(x=\"bucket_name\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot history function.\n",
    "def plotresult(History):\n",
    "    %matplotlib inline\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(History.history['acc'])\n",
    "\n",
    "    plt.ylabel('Accuracy %')\n",
    "    plt.title('Training')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(History.history['val_acc'])\n",
    "    plt.title('Validation')\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.plot(History.history['loss'])\n",
    "    plt.ylabel('categorical_crossentropy Training Loss')\n",
    "    plt.xlabel('epochs')\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.plot(History.history['val_loss'])\n",
    "    plt.xlabel('epochs')\n",
    "    \n",
    "def get_token_for_sentence(corpus):\n",
    "#     corpus = df.description.tolist()\n",
    "\n",
    "\n",
    "    vocabulary_size = 20000\n",
    "    tokenizer = Tokenizer(num_words= vocabulary_size, \n",
    "                          filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                          lower=True, \n",
    "                          split=' ')\n",
    "\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    top_word = len(tokenizer.index_word) +1\n",
    "    print(\"vocab:\",top_word)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: 7427\n",
      "vocab: 21117\n"
     ]
    }
   ],
   "source": [
    "name_token = get_token_for_sentence(df.product_name.tolist())\n",
    "des_token = get_token_for_sentence(df.description.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "\n",
    "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "\n",
    "    Example: adding bi-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "\n",
    "    Example: adding tri-gram\n",
    "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(x_train, x_test, y_train, y_test,maxlen,fold_number,ngram_range,feature):\n",
    "    current_model_name = \"model/\"+feature+\"_ngram_\"+str(ngram_range) + \"_fold_\"+str(fold_number)+\"_best.h5\"\n",
    "\n",
    "\n",
    "    max_features = 20000\n",
    "#     maxlen = 50\n",
    "    batch_size = 32\n",
    "    embedding_dims = 50\n",
    "    epochs = 35\n",
    "    Patience = 3\n",
    "    \n",
    "    print(len(x_train), 'train sequences')\n",
    "    print(len(x_test), 'test sequences')\n",
    "    print('Average train sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_train)))))\n",
    "    print('Average test sequence length: {}'.format(\n",
    "        np.mean(list(map(len, x_test)))))\n",
    "\n",
    "    if ngram_range > 1:\n",
    "        print('Adding {}-gram features'.format(ngram_range))\n",
    "        # Create set of unique n-gram from the training set.\n",
    "        ngram_set = set()\n",
    "        for input_list in x_train:\n",
    "            for i in range(2, ngram_range + 1):\n",
    "                set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "                ngram_set.update(set_of_ngram)\n",
    "\n",
    "        # Dictionary mapping n-gram token to a unique integer.\n",
    "        # Integer values are greater than max_features in order\n",
    "        # to avoid collision with existing features.\n",
    "        start_index = max_features + 1\n",
    "        token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "        indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "        # max_features is the highest integer that could be found in the dataset.\n",
    "        max_features = np.max(list(indice_token.keys())) + 1\n",
    "        print(\"max_features:\",max_features)\n",
    "\n",
    "        # Augmenting x_train and x_test with n-grams features\n",
    "        x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "        x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "        print('Average train sequence length: {}'.format(\n",
    "            np.mean(list(map(len, x_train)), dtype=int)))\n",
    "        print('Average test sequence length: {}'.format(\n",
    "            np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "    print('Pad sequences (samples x time)')\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('x_test shape:', x_test.shape)\n",
    "\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_features,\n",
    "                        embedding_dims,\n",
    "                        input_length=maxlen))\n",
    "\n",
    "    # we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "    # of all words in the document\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(NUM_OF_CLASS, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # print(model.summary())\n",
    "    checkpoint = ModelCheckpoint(current_model_name, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=Patience),checkpoint], \n",
    "              verbose=1 )\n",
    "    \n",
    "    model.load_weights(current_model_name)\n",
    "#     yhat = np.argmax(model.predict(x_test),axis=1)\n",
    "#     return f1_score(y_test,yhat,average='macro')\n",
    "    yhat = model.predict(x_test)\n",
    "    return yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_model_fusion(x_train, x_test, y_train, y_test,maxlen,fold_number,ngram_range,feature):\n",
    "#     current_model_name = \"model/\"+feature+\"_ngram_\"+str(ngram_range) + \"_fold_\"+str(fold_number)+\"_best.h5\"\n",
    "\n",
    "\n",
    "#     max_features = 20000\n",
    "# #     maxlen = 50\n",
    "#     batch_size = 32\n",
    "#     embedding_dims = 50\n",
    "#     epochs = 35\n",
    "#     Patience = 3\n",
    "    \n",
    "#     print(len(x_train), 'train sequences')\n",
    "#     print(len(x_test), 'test sequences')\n",
    "#     print('Average train sequence length: {}'.format(\n",
    "#         np.mean(list(map(len, x_train)))))\n",
    "#     print('Average test sequence length: {}'.format(\n",
    "#         np.mean(list(map(len, x_test)))))\n",
    "\n",
    "#     if ngram_range > 1:\n",
    "#         print('Adding {}-gram features'.format(ngram_range))\n",
    "#         # Create set of unique n-gram from the training set.\n",
    "#         ngram_set = set()\n",
    "#         for input_list in x_train:\n",
    "#             for i in range(2, ngram_range + 1):\n",
    "#                 set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "#                 ngram_set.update(set_of_ngram)\n",
    "\n",
    "#         # Dictionary mapping n-gram token to a unique integer.\n",
    "#         # Integer values are greater than max_features in order\n",
    "#         # to avoid collision with existing features.\n",
    "#         start_index = max_features + 1\n",
    "#         token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "#         indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "#         # max_features is the highest integer that could be found in the dataset.\n",
    "#         max_features = np.max(list(indice_token.keys())) + 1\n",
    "#         print(\"max_features:\",max_features)\n",
    "\n",
    "#         # Augmenting x_train and x_test with n-grams features\n",
    "#         x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "#         x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "#         print('Average train sequence length: {}'.format(\n",
    "#             np.mean(list(map(len, x_train)), dtype=int)))\n",
    "#         print('Average test sequence length: {}'.format(\n",
    "#             np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "#     print('Pad sequences (samples x time)')\n",
    "#     x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "#     x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "#     print('x_train shape:', x_train.shape)\n",
    "#     print('x_test shape:', x_test.shape)\n",
    "\n",
    "#     print('Build model...')\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # we start off with an efficient embedding layer which maps\n",
    "#     # our vocab indices into embedding_dims dimensions\n",
    "#     model.add(Embedding(max_features,\n",
    "#                         embedding_dims,\n",
    "#                         input_length=maxlen))\n",
    "\n",
    "#     # we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "#     # of all words in the document\n",
    "#     model.add(GlobalAveragePooling1D())\n",
    "\n",
    "#     # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "#     model.add(Dense(NUM_OF_CLASS, activation='softmax'))\n",
    "\n",
    "#     model.compile(loss='sparse_categorical_crossentropy',\n",
    "#                   optimizer='adam',\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "#     # print(model.summary())\n",
    "#     checkpoint = ModelCheckpoint(current_model_name, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "#     history = model.fit(x_train, y_train,\n",
    "#               batch_size=batch_size,\n",
    "#               epochs=epochs,\n",
    "#               validation_data=(x_test, y_test),\n",
    "#               callbacks=[EarlyStopping(monitor='val_loss', patience=Patience),checkpoint], \n",
    "#               verbose=1 )\n",
    "    \n",
    "#     model.load_weights(current_model_name)\n",
    "# #     yhat = np.argmax(model.predict(x_test),axis=1)\n",
    "# #     return f1_score(y_test,yhat,average='macro')\n",
    "#     yhat = model.predict(x_test)\n",
    "#     return yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in Fold  1\n",
      "27039 train sequences\n",
      "6803 test sequences\n",
      "Average train sequence length: 6.373719442287067\n",
      "Average test sequence length: 6.693076583860062\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (27039, 16)\n",
      "x_test shape: (6803, 16)\n",
      "Build model...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 27039 samples, validate on 6803 samples\n",
      "Epoch 1/35\n",
      "27039/27039 [==============================] - 5s 170us/step - loss: 3.1961 - acc: 0.2850 - val_loss: 2.6096 - val_acc: 0.4524\n",
      "Epoch 2/35\n",
      "27039/27039 [==============================] - 3s 103us/step - loss: 1.8510 - acc: 0.6503 - val_loss: 1.6897 - val_acc: 0.6746\n",
      "Epoch 3/35\n",
      "27039/27039 [==============================] - 3s 95us/step - loss: 1.1123 - acc: 0.7726 - val_loss: 1.2490 - val_acc: 0.7561\n",
      "Epoch 4/35\n",
      "27039/27039 [==============================] - 3s 95us/step - loss: 0.7688 - acc: 0.8410 - val_loss: 1.0154 - val_acc: 0.7958\n",
      "Epoch 5/35\n",
      "27039/27039 [==============================] - 3s 94us/step - loss: 0.5637 - acc: 0.8862 - val_loss: 0.8683 - val_acc: 0.8268\n",
      "Epoch 6/35\n",
      "27039/27039 [==============================] - 3s 97us/step - loss: 0.4279 - acc: 0.9169 - val_loss: 0.7633 - val_acc: 0.8467\n",
      "Epoch 7/35\n",
      "27039/27039 [==============================] - 3s 97us/step - loss: 0.3322 - acc: 0.9381 - val_loss: 0.6945 - val_acc: 0.8671\n",
      "Epoch 8/35\n",
      "27039/27039 [==============================] - 3s 95us/step - loss: 0.2638 - acc: 0.9504 - val_loss: 0.6502 - val_acc: 0.8739\n",
      "Epoch 9/35\n",
      "27039/27039 [==============================] - 3s 94us/step - loss: 0.2146 - acc: 0.9589 - val_loss: 0.6185 - val_acc: 0.8799\n",
      "Epoch 10/35\n",
      "27039/27039 [==============================] - 3s 94us/step - loss: 0.1782 - acc: 0.9643 - val_loss: 0.5870 - val_acc: 0.8853\n",
      "Epoch 11/35\n",
      "27039/27039 [==============================] - 3s 97us/step - loss: 0.1504 - acc: 0.9687 - val_loss: 0.5650 - val_acc: 0.8878\n",
      "Epoch 12/35\n",
      "27039/27039 [==============================] - 3s 94us/step - loss: 0.1285 - acc: 0.9729 - val_loss: 0.5495 - val_acc: 0.8918\n",
      "Epoch 13/35\n",
      "27039/27039 [==============================] - 3s 95us/step - loss: 0.1112 - acc: 0.9761 - val_loss: 0.5371 - val_acc: 0.8899\n",
      "Epoch 14/35\n",
      "27039/27039 [==============================] - 3s 94us/step - loss: 0.0970 - acc: 0.9795 - val_loss: 0.5276 - val_acc: 0.8956\n",
      "Epoch 15/35\n",
      "27039/27039 [==============================] - 3s 95us/step - loss: 0.0853 - acc: 0.9822 - val_loss: 0.5195 - val_acc: 0.8983\n",
      "Epoch 16/35\n",
      "27039/27039 [==============================] - 3s 95us/step - loss: 0.0755 - acc: 0.9838 - val_loss: 0.5197 - val_acc: 0.8983\n",
      "Epoch 17/35\n",
      "27039/27039 [==============================] - 3s 94us/step - loss: 0.0670 - acc: 0.9858 - val_loss: 0.5186 - val_acc: 0.9000\n",
      "Epoch 18/35\n",
      "27039/27039 [==============================] - 3s 95us/step - loss: 0.0597 - acc: 0.9869 - val_loss: 0.5171 - val_acc: 0.9012\n",
      "Epoch 19/35\n",
      "27039/27039 [==============================] - 3s 95us/step - loss: 0.0533 - acc: 0.9887 - val_loss: 0.5216 - val_acc: 0.9005\n",
      "Epoch 20/35\n",
      "27039/27039 [==============================] - 3s 98us/step - loss: 0.0479 - acc: 0.9895 - val_loss: 0.5155 - val_acc: 0.9030\n",
      "Epoch 21/35\n",
      "27039/27039 [==============================] - 3s 108us/step - loss: 0.0429 - acc: 0.9906 - val_loss: 0.5279 - val_acc: 0.9003\n",
      "Epoch 22/35\n",
      "27039/27039 [==============================] - 3s 106us/step - loss: 0.0388 - acc: 0.9915 - val_loss: 0.5231 - val_acc: 0.9014\n",
      "Epoch 23/35\n",
      "27039/27039 [==============================] - 3s 98us/step - loss: 0.0349 - acc: 0.9922 - val_loss: 0.5198 - val_acc: 0.9022\n",
      "27039 train sequences\n",
      "6803 test sequences\n",
      "Average train sequence length: 59.26373016753578\n",
      "Average test sequence length: 70.28002351903572\n",
      "Adding 2-gram features\n",
      "max_features: 231974\n",
      "Average train sequence length: 117\n",
      "Average test sequence length: 128\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (27039, 50)\n",
      "x_test shape: (6803, 50)\n",
      "Build model...\n",
      "Train on 27039 samples, validate on 6803 samples\n",
      "Epoch 1/35\n",
      "27039/27039 [==============================] - 9s 349us/step - loss: 3.0998 - acc: 0.3510 - val_loss: 2.5653 - val_acc: 0.4088\n",
      "Epoch 2/35\n",
      "27039/27039 [==============================] - 9s 323us/step - loss: 1.7966 - acc: 0.5787 - val_loss: 2.0452 - val_acc: 0.5068\n",
      "Epoch 3/35\n",
      "27039/27039 [==============================] - 9s 323us/step - loss: 1.2809 - acc: 0.6982 - val_loss: 1.8022 - val_acc: 0.5645\n",
      "Epoch 4/35\n",
      "27039/27039 [==============================] - 9s 322us/step - loss: 0.9829 - acc: 0.7660 - val_loss: 1.6543 - val_acc: 0.5990\n",
      "Epoch 5/35\n",
      "27039/27039 [==============================] - 9s 322us/step - loss: 0.7772 - acc: 0.8142 - val_loss: 1.5591 - val_acc: 0.6224\n",
      "Epoch 6/35\n",
      "27039/27039 [==============================] - 9s 324us/step - loss: 0.6295 - acc: 0.8473 - val_loss: 1.4981 - val_acc: 0.6410\n",
      "Epoch 7/35\n",
      "27039/27039 [==============================] - 9s 324us/step - loss: 0.5227 - acc: 0.8723 - val_loss: 1.4555 - val_acc: 0.6490\n",
      "Epoch 8/35\n",
      "27039/27039 [==============================] - 9s 323us/step - loss: 0.4472 - acc: 0.8850 - val_loss: 1.4312 - val_acc: 0.6604\n",
      "Epoch 9/35\n",
      "27039/27039 [==============================] - 9s 323us/step - loss: 0.3945 - acc: 0.8929 - val_loss: 1.4133 - val_acc: 0.6629\n",
      "Epoch 10/35\n",
      "27039/27039 [==============================] - 9s 324us/step - loss: 0.3583 - acc: 0.8962 - val_loss: 1.4044 - val_acc: 0.6610\n",
      "Epoch 11/35\n",
      "27039/27039 [==============================] - 9s 322us/step - loss: 0.3334 - acc: 0.9000 - val_loss: 1.3965 - val_acc: 0.6696\n",
      "Epoch 12/35\n",
      "27039/27039 [==============================] - 9s 322us/step - loss: 0.3160 - acc: 0.9017 - val_loss: 1.3938 - val_acc: 0.6690\n",
      "Epoch 13/35\n",
      "27039/27039 [==============================] - 9s 322us/step - loss: 0.3034 - acc: 0.9046 - val_loss: 1.3945 - val_acc: 0.6712\n",
      "Epoch 14/35\n",
      "27039/27039 [==============================] - 9s 324us/step - loss: 0.2942 - acc: 0.9048 - val_loss: 1.4014 - val_acc: 0.6712\n",
      "Epoch 15/35\n",
      "27039/27039 [==============================] - 9s 323us/step - loss: 0.2871 - acc: 0.9064 - val_loss: 1.4038 - val_acc: 0.6724\n",
      "                                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                                            aprons       1.00      0.93      0.96        44\n",
      "                                                     baking dishes       0.98      0.93      0.96        69\n",
      "                                       baking sheets & baking pans       0.81      0.61      0.70        36\n",
      "                                                          blenders       1.00      0.97      0.98        30\n",
      "                                         bottle stoppers & pourers       0.86      0.52      0.65        23\n",
      "                                                             bowls       0.89      0.93      0.91       389\n",
      "                                   chopping/slicing/cutting boards       0.93      0.89      0.91        61\n",
      "                                                          coasters       1.00      1.00      1.00        69\n",
      "                                        cocktail preparation tools       0.87      0.85      0.86        40\n",
      "                                    coffee, tea, & espresso makers       0.92      0.99      0.95        83\n",
      "                         coffee, tea, & espresso preparation tools       0.92      0.57      0.71        21\n",
      "                                             colanders & strainers       1.00      0.86      0.92        14\n",
      "                                    cooking utensils variety packs       0.75      0.56      0.64        32\n",
      "                                  cooking/baking spoons & spatulas       0.87      0.87      0.87        38\n",
      "               cookware & bakeware replacement parts & accessories       0.50      0.20      0.29        15\n",
      "                                 cookware & bakeware storage racks       1.00      0.52      0.68        27\n",
      "                                 cookware & bakeware variety packs       0.89      0.93      0.91        44\n",
      "                                           coolers & food carriers       0.97      0.78      0.87       222\n",
      "                                                 cruets & sprayers       0.88      0.67      0.76        21\n",
      "                                                         decanters       0.56      0.90      0.69        41\n",
      "                                          dish drying racks & mats       0.71      1.00      0.83        22\n",
      "                                                        dispensers       1.00      0.72      0.84        29\n",
      "                                           disposable food storage       0.60      0.19      0.29        16\n",
      "                                              dough & baking tools       1.00      0.94      0.97        18\n",
      "                                            dutch ovens & braisers       0.84      1.00      0.91        51\n",
      "                                                     flatware sets       0.95      0.91      0.93       120\n",
      "food & beverage preparation device accessories & replacement parts       0.71      0.52      0.60        23\n",
      "                  food decorating tools & presentation accessories       0.45      0.33      0.38        15\n",
      "                                     food preparation/mixing bowls       0.54      0.62      0.58        24\n",
      "                                                   food processors       1.00      0.94      0.97        17\n",
      "                                              food storage baskets       0.82      0.82      0.82        17\n",
      "                   food storage canisters/containers & accessories       0.64      1.00      0.78       307\n",
      "                                                             forks       1.00      0.88      0.93        16\n",
      "                                            frying pans & skillets       0.88      1.00      0.93        70\n",
      "                                                           glasses       0.95      0.95      0.95       867\n",
      "                                             grill pans & griddles       1.00      1.00      1.00        27\n",
      "                                 grinders, juicers, & ice crushers       0.84      0.95      0.89        22\n",
      "                                ice buckets & wine bottle chillers       0.87      0.92      0.89        49\n",
      "                                 kitchen merchandise variety packs       0.94      0.78      0.85        58\n",
      "       kitchen textiles & clothing replacement parts & accessories       1.00      1.00      1.00        77\n",
      "                                   kitchen/dishtowels & dishcloths       0.98      0.96      0.97       159\n",
      "                                      knife blocks, racks, & rolls       0.78      0.41      0.54        17\n",
      "                                                            knives       0.86      0.96      0.91        96\n",
      "                                                    measuring cups       1.00      0.87      0.93        15\n",
      "                                                            mixers       1.00      0.98      0.99        42\n",
      "                                                 mugs/teacups/cups       0.91      0.86      0.89       247\n",
      "                                                           napkins       0.99      0.97      0.98       228\n",
      "                                                           openers       1.00      0.89      0.94        47\n",
      "                                       peelers, slicers, & graters       0.82      0.75      0.78        24\n",
      "                                                pitchers & carafes       0.86      0.75      0.80        57\n",
      "                                                         placemats       0.99      0.98      0.98       215\n",
      "                                                 plates & platters       0.94      0.98      0.96       552\n",
      "                                           potholders & oven mitts       0.89      0.80      0.84        40\n",
      "                                        reusable food storage bags       0.79      0.92      0.85        12\n",
      "                                                         saucepans       0.91      0.81      0.86        26\n",
      "                                         seasoning shakers & mills       0.89      0.93      0.91        44\n",
      "                                                  serving utensils       0.69      0.50      0.58        22\n",
      "                          serving utensils & cutlery variety packs       0.64      0.44      0.52        16\n",
      "                                                    shaker bottles       1.00      0.50      0.67        32\n",
      "                                           slow & pressure cookers       0.88      1.00      0.93        14\n",
      "                                          specialty serving dishes       0.76      0.42      0.54        31\n",
      "                                                       spice racks       0.34      0.75      0.47        16\n",
      "                                                       spoon rests       1.00      0.86      0.92        14\n",
      "                                                            spoons       0.83      0.86      0.85        29\n",
      "                                                            stands       0.89      0.74      0.81        34\n",
      "                                      steamers, pasta & stock pots       0.79      0.88      0.83        17\n",
      "                                       tablecloths & table runners       1.00      0.99      0.99       392\n",
      "                                   tableware storage & accessories       0.61      0.92      0.73        12\n",
      "                                           tableware variety packs       0.98      0.84      0.91       701\n",
      "                                              teapots & teakettles       1.00      0.96      0.98        45\n",
      "                                          toasters & toaster ovens       0.97      1.00      0.98        28\n",
      "                                                    trays & boards       0.79      0.78      0.78       174\n",
      "                                                           trivets       0.81      1.00      0.90        22\n",
      "                                       turntables/lazy susan trays       0.63      0.92      0.75        13\n",
      "                                            utensil holders/crocks       0.71      0.57      0.63        21\n",
      "                                vacuum/water bottles & travel mugs       0.78      0.96      0.86       157\n",
      "                                       wine racks & bottle holders       0.96      0.82      0.88        28\n",
      "\n",
      "                                                          accuracy                           0.90      6803\n",
      "                                                         macro avg       0.86      0.81      0.82      6803\n",
      "                                                      weighted avg       0.91      0.90      0.90      6803\n",
      "\n",
      "in Fold  2\n",
      "27064 train sequences\n",
      "6778 test sequences\n",
      "Average train sequence length: 6.339195979899498\n",
      "Average test sequence length: 6.832103865447035\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (27064, 16)\n",
      "x_test shape: (6778, 16)\n",
      "Build model...\n",
      "Train on 27064 samples, validate on 6778 samples\n",
      "Epoch 1/35\n",
      "27064/27064 [==============================] - 3s 106us/step - loss: 3.2288 - acc: 0.2639 - val_loss: 2.5893 - val_acc: 0.4875\n",
      "Epoch 2/35\n",
      "27064/27064 [==============================] - 3s 96us/step - loss: 1.9253 - acc: 0.6384 - val_loss: 1.5359 - val_acc: 0.7117\n",
      "Epoch 3/35\n",
      "27064/27064 [==============================] - 3s 97us/step - loss: 1.1706 - acc: 0.7612 - val_loss: 1.0892 - val_acc: 0.7909\n",
      "Epoch 4/35\n",
      "27064/27064 [==============================] - 3s 99us/step - loss: 0.8174 - acc: 0.8346 - val_loss: 0.8425 - val_acc: 0.8339\n",
      "Epoch 5/35\n",
      "27064/27064 [==============================] - 3s 95us/step - loss: 0.6039 - acc: 0.8737 - val_loss: 0.6901 - val_acc: 0.8609\n",
      "Epoch 6/35\n",
      "27064/27064 [==============================] - 3s 95us/step - loss: 0.4616 - acc: 0.9058 - val_loss: 0.5915 - val_acc: 0.8764\n",
      "Epoch 7/35\n",
      "27064/27064 [==============================] - 3s 96us/step - loss: 0.3605 - acc: 0.9297 - val_loss: 0.5206 - val_acc: 0.8896\n",
      "Epoch 8/35\n",
      "27064/27064 [==============================] - 3s 95us/step - loss: 0.2868 - acc: 0.9454 - val_loss: 0.4733 - val_acc: 0.8972\n",
      "Epoch 9/35\n",
      "27064/27064 [==============================] - 3s 95us/step - loss: 0.2325 - acc: 0.9553 - val_loss: 0.4399 - val_acc: 0.9045\n",
      "Epoch 10/35\n",
      "27064/27064 [==============================] - 3s 96us/step - loss: 0.1924 - acc: 0.9619 - val_loss: 0.4154 - val_acc: 0.9071\n",
      "Epoch 11/35\n",
      "27064/27064 [==============================] - 3s 96us/step - loss: 0.1620 - acc: 0.9667 - val_loss: 0.4005 - val_acc: 0.9088\n",
      "Epoch 12/35\n",
      "27064/27064 [==============================] - 3s 96us/step - loss: 0.1384 - acc: 0.9711 - val_loss: 0.3848 - val_acc: 0.9100\n",
      "Epoch 13/35\n",
      "27064/27064 [==============================] - 3s 96us/step - loss: 0.1195 - acc: 0.9747 - val_loss: 0.3798 - val_acc: 0.9075\n",
      "Epoch 14/35\n",
      "27064/27064 [==============================] - 3s 97us/step - loss: 0.1042 - acc: 0.9775 - val_loss: 0.3750 - val_acc: 0.9065\n",
      "Epoch 15/35\n",
      "27064/27064 [==============================] - 3s 96us/step - loss: 0.0917 - acc: 0.9799 - val_loss: 0.3761 - val_acc: 0.9028\n",
      "Epoch 16/35\n",
      "27064/27064 [==============================] - 3s 96us/step - loss: 0.0811 - acc: 0.9823 - val_loss: 0.3730 - val_acc: 0.9034\n",
      "Epoch 17/35\n",
      "27064/27064 [==============================] - 3s 96us/step - loss: 0.0721 - acc: 0.9842 - val_loss: 0.3658 - val_acc: 0.9042\n",
      "Epoch 18/35\n",
      "27064/27064 [==============================] - 3s 100us/step - loss: 0.0645 - acc: 0.9858 - val_loss: 0.3746 - val_acc: 0.9004\n",
      "Epoch 19/35\n",
      "27064/27064 [==============================] - 3s 98us/step - loss: 0.0578 - acc: 0.9870 - val_loss: 0.3768 - val_acc: 0.9022\n",
      "Epoch 20/35\n",
      "27064/27064 [==============================] - 3s 102us/step - loss: 0.0521 - acc: 0.9885 - val_loss: 0.3757 - val_acc: 0.9040\n",
      "27064 train sequences\n",
      "6778 test sequences\n",
      "Average train sequence length: 62.32286432160804\n",
      "Average test sequence length: 58.10578341693715\n",
      "Adding 2-gram features\n",
      "max_features: 239163\n",
      "Average train sequence length: 123\n",
      "Average test sequence length: 105\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (27064, 50)\n",
      "x_test shape: (6778, 50)\n",
      "Build model...\n",
      "Train on 27064 samples, validate on 6778 samples\n",
      "Epoch 1/35\n",
      "27064/27064 [==============================] - 10s 359us/step - loss: 3.0971 - acc: 0.3500 - val_loss: 2.5910 - val_acc: 0.3963\n",
      "Epoch 2/35\n",
      "27064/27064 [==============================] - 9s 332us/step - loss: 1.7769 - acc: 0.5878 - val_loss: 2.1134 - val_acc: 0.4973\n",
      "Epoch 3/35\n",
      "27064/27064 [==============================] - 9s 332us/step - loss: 1.2518 - acc: 0.7072 - val_loss: 1.8879 - val_acc: 0.5510\n",
      "Epoch 4/35\n",
      "27064/27064 [==============================] - 9s 331us/step - loss: 0.9368 - acc: 0.7867 - val_loss: 1.7563 - val_acc: 0.5760\n",
      "Epoch 5/35\n",
      "27064/27064 [==============================] - 9s 331us/step - loss: 0.7196 - acc: 0.8371 - val_loss: 1.6654 - val_acc: 0.5947\n",
      "Epoch 6/35\n",
      "27064/27064 [==============================] - 9s 333us/step - loss: 0.5644 - acc: 0.8703 - val_loss: 1.6208 - val_acc: 0.6050\n",
      "Epoch 7/35\n",
      "27064/27064 [==============================] - 9s 332us/step - loss: 0.4552 - acc: 0.8927 - val_loss: 1.5967 - val_acc: 0.6133\n",
      "Epoch 8/35\n",
      "27064/27064 [==============================] - 9s 331us/step - loss: 0.3803 - acc: 0.9054 - val_loss: 1.5853 - val_acc: 0.6146\n",
      "Epoch 9/35\n",
      "27064/27064 [==============================] - 9s 331us/step - loss: 0.3290 - acc: 0.9120 - val_loss: 1.5782 - val_acc: 0.6115\n",
      "Epoch 10/35\n",
      "27064/27064 [==============================] - 9s 330us/step - loss: 0.2942 - acc: 0.9147 - val_loss: 1.5704 - val_acc: 0.6099\n",
      "Epoch 11/35\n",
      "27064/27064 [==============================] - 9s 331us/step - loss: 0.2701 - acc: 0.9187 - val_loss: 1.5776 - val_acc: 0.6140\n",
      "Epoch 12/35\n",
      "27064/27064 [==============================] - 9s 331us/step - loss: 0.2529 - acc: 0.9214 - val_loss: 1.5830 - val_acc: 0.6107\n",
      "Epoch 13/35\n",
      "27064/27064 [==============================] - 9s 332us/step - loss: 0.2404 - acc: 0.9238 - val_loss: 1.5991 - val_acc: 0.5975\n",
      "                                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                                            aprons       0.98      0.98      0.98        44\n",
      "                                                     baking dishes       0.89      0.94      0.92        69\n",
      "                                       baking sheets & baking pans       0.85      0.63      0.72        35\n",
      "                                                          blenders       0.91      1.00      0.95        30\n",
      "                                         bottle stoppers & pourers       0.87      0.91      0.89        22\n",
      "                                                             bowls       0.89      0.96      0.92       389\n",
      "                                   chopping/slicing/cutting boards       0.98      0.95      0.97        61\n",
      "                                                          coasters       0.99      1.00      0.99        68\n",
      "                                        cocktail preparation tools       0.89      0.80      0.84        40\n",
      "                                    coffee, tea, & espresso makers       0.95      0.90      0.93        83\n",
      "                         coffee, tea, & espresso preparation tools       0.83      0.24      0.37        21\n",
      "                                             colanders & strainers       1.00      0.64      0.78        14\n",
      "                                    cooking utensils variety packs       0.64      0.29      0.40        31\n",
      "                                  cooking/baking spoons & spatulas       0.69      0.71      0.70        38\n",
      "               cookware & bakeware replacement parts & accessories       1.00      0.64      0.78        14\n",
      "                                 cookware & bakeware storage racks       0.81      0.85      0.83        26\n",
      "                                 cookware & bakeware variety packs       0.80      0.86      0.83        43\n",
      "                                           coolers & food carriers       0.98      0.89      0.93       222\n",
      "                                                 cruets & sprayers       0.92      0.55      0.69        20\n",
      "                                                         decanters       0.97      0.90      0.94        40\n",
      "                                          dish drying racks & mats       0.87      0.91      0.89        22\n",
      "                                                        dispensers       0.97      1.00      0.98        29\n",
      "                                           disposable food storage       1.00      0.53      0.70        15\n",
      "                                              dough & baking tools       1.00      0.83      0.91        18\n",
      "                                            dutch ovens & braisers       0.91      0.98      0.94        50\n",
      "                                                     flatware sets       0.92      0.81      0.86       120\n",
      "food & beverage preparation device accessories & replacement parts       0.79      0.96      0.86        23\n",
      "                  food decorating tools & presentation accessories       1.00      0.60      0.75        15\n",
      "                                     food preparation/mixing bowls       0.88      0.62      0.73        24\n",
      "                                                   food processors       1.00      1.00      1.00        17\n",
      "                                              food storage baskets       0.67      0.47      0.55        17\n",
      "                   food storage canisters/containers & accessories       0.89      0.97      0.93       306\n",
      "                                                             forks       0.93      0.88      0.90        16\n",
      "                                            frying pans & skillets       0.86      0.90      0.88        70\n",
      "                                                           glasses       0.94      0.96      0.95       867\n",
      "                                             grill pans & griddles       0.73      1.00      0.84        27\n",
      "                                 grinders, juicers, & ice crushers       0.93      0.64      0.76        22\n",
      "                                ice buckets & wine bottle chillers       0.91      0.88      0.90        49\n",
      "                                 kitchen merchandise variety packs       0.43      0.60      0.50        58\n",
      "       kitchen textiles & clothing replacement parts & accessories       0.99      1.00      0.99        77\n",
      "                                   kitchen/dishtowels & dishcloths       0.98      1.00      0.99       158\n",
      "                                      knife blocks, racks, & rolls       1.00      1.00      1.00        16\n",
      "                                                            knives       0.85      0.97      0.91        95\n",
      "                                                    measuring cups       1.00      0.93      0.97        15\n",
      "                                                            mixers       1.00      1.00      1.00        42\n",
      "                                                 mugs/teacups/cups       0.94      0.97      0.96       247\n",
      "                                                           napkins       1.00      1.00      1.00       228\n",
      "                                                           openers       0.96      1.00      0.98        47\n",
      "                                       peelers, slicers, & graters       0.91      0.87      0.89        23\n",
      "                                                pitchers & carafes       0.94      0.89      0.92        57\n",
      "                                                         placemats       0.90      1.00      0.95       214\n",
      "                                                 plates & platters       0.94      0.98      0.96       552\n",
      "                                           potholders & oven mitts       0.95      0.95      0.95        40\n",
      "                                        reusable food storage bags       1.00      0.75      0.86        12\n",
      "                                                         saucepans       0.90      0.69      0.78        26\n",
      "                                         seasoning shakers & mills       0.92      1.00      0.96        44\n",
      "                                                  serving utensils       0.71      0.55      0.62        22\n",
      "                          serving utensils & cutlery variety packs       0.67      0.27      0.38        15\n",
      "                                                    shaker bottles       0.91      0.94      0.92        32\n",
      "                                           slow & pressure cookers       0.75      0.86      0.80        14\n",
      "                                          specialty serving dishes       0.91      0.32      0.48        31\n",
      "                                                       spice racks       0.85      0.69      0.76        16\n",
      "                                                       spoon rests       1.00      0.86      0.92        14\n",
      "                                                            spoons       0.71      0.86      0.78        29\n",
      "                                                            stands       0.81      0.88      0.84        33\n",
      "                                      steamers, pasta & stock pots       0.88      0.82      0.85        17\n",
      "                                       tablecloths & table runners       0.99      1.00      1.00       392\n",
      "                                   tableware storage & accessories       1.00      0.91      0.95        11\n",
      "                                           tableware variety packs       0.94      0.87      0.90       700\n",
      "                                              teapots & teakettles       0.96      1.00      0.98        45\n",
      "                                          toasters & toaster ovens       1.00      0.86      0.92        28\n",
      "                                                    trays & boards       0.85      0.96      0.90       173\n",
      "                                                           trivets       0.95      0.90      0.93        21\n",
      "                                       turntables/lazy susan trays       0.92      0.85      0.88        13\n",
      "                                            utensil holders/crocks       0.80      0.76      0.78        21\n",
      "                                vacuum/water bottles & travel mugs       0.83      0.82      0.83       156\n",
      "                                       wine racks & bottle holders       0.87      0.96      0.91        27\n",
      "\n",
      "                                                          accuracy                           0.92      6778\n",
      "                                                         macro avg       0.90      0.83      0.85      6778\n",
      "                                                      weighted avg       0.92      0.92      0.91      6778\n",
      "\n",
      "in Fold  3\n",
      "27074 train sequences\n",
      "6768 test sequences\n",
      "Average train sequence length: 6.495013666248061\n",
      "Average test sequence length: 6.20951536643026\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (27074, 16)\n",
      "x_test shape: (6768, 16)\n",
      "Build model...\n",
      "Train on 27074 samples, validate on 6768 samples\n",
      "Epoch 1/35\n",
      "27074/27074 [==============================] - 3s 108us/step - loss: 3.2013 - acc: 0.2808 - val_loss: 2.6509 - val_acc: 0.4512\n",
      "Epoch 2/35\n",
      "27074/27074 [==============================] - 3s 97us/step - loss: 1.8979 - acc: 0.6400 - val_loss: 1.6374 - val_acc: 0.6909\n",
      "Epoch 3/35\n",
      "27074/27074 [==============================] - 3s 95us/step - loss: 1.1676 - acc: 0.7617 - val_loss: 1.1636 - val_acc: 0.7784\n",
      "Epoch 4/35\n",
      "27074/27074 [==============================] - 3s 95us/step - loss: 0.8272 - acc: 0.8317 - val_loss: 0.8995 - val_acc: 0.8113\n",
      "Epoch 5/35\n",
      "27074/27074 [==============================] - 3s 96us/step - loss: 0.6186 - acc: 0.8697 - val_loss: 0.7272 - val_acc: 0.8462\n",
      "Epoch 6/35\n",
      "27074/27074 [==============================] - 3s 99us/step - loss: 0.4763 - acc: 0.9008 - val_loss: 0.6107 - val_acc: 0.8722\n",
      "Epoch 7/35\n",
      "27074/27074 [==============================] - 3s 96us/step - loss: 0.3751 - acc: 0.9237 - val_loss: 0.5344 - val_acc: 0.8896\n",
      "Epoch 8/35\n",
      "27074/27074 [==============================] - 3s 98us/step - loss: 0.3011 - acc: 0.9409 - val_loss: 0.4832 - val_acc: 0.8933\n",
      "Epoch 9/35\n",
      "27074/27074 [==============================] - 3s 97us/step - loss: 0.2460 - acc: 0.9522 - val_loss: 0.4492 - val_acc: 0.8998\n",
      "Epoch 10/35\n",
      "27074/27074 [==============================] - 3s 97us/step - loss: 0.2048 - acc: 0.9591 - val_loss: 0.4243 - val_acc: 0.9026\n",
      "Epoch 11/35\n",
      "27074/27074 [==============================] - 3s 96us/step - loss: 0.1731 - acc: 0.9650 - val_loss: 0.4120 - val_acc: 0.9050\n",
      "Epoch 12/35\n",
      "27074/27074 [==============================] - 3s 96us/step - loss: 0.1485 - acc: 0.9695 - val_loss: 0.4052 - val_acc: 0.9065\n",
      "Epoch 13/35\n",
      "27074/27074 [==============================] - 3s 96us/step - loss: 0.1287 - acc: 0.9729 - val_loss: 0.4040 - val_acc: 0.9062\n",
      "Epoch 14/35\n",
      "27074/27074 [==============================] - 3s 96us/step - loss: 0.1128 - acc: 0.9761 - val_loss: 0.4020 - val_acc: 0.9072\n",
      "Epoch 15/35\n",
      "27074/27074 [==============================] - 3s 96us/step - loss: 0.0995 - acc: 0.9787 - val_loss: 0.4020 - val_acc: 0.9078\n",
      "Epoch 16/35\n",
      "27074/27074 [==============================] - 3s 96us/step - loss: 0.0883 - acc: 0.9804 - val_loss: 0.4027 - val_acc: 0.9078\n",
      "Epoch 17/35\n",
      "27074/27074 [==============================] - 3s 96us/step - loss: 0.0788 - acc: 0.9824 - val_loss: 0.4107 - val_acc: 0.9082\n",
      "27074 train sequences\n",
      "6768 test sequences\n",
      "Average train sequence length: 62.12772401566078\n",
      "Average test sequence length: 58.880171394799056\n",
      "Adding 2-gram features\n",
      "max_features: 241968\n",
      "Average train sequence length: 123\n",
      "Average test sequence length: 107\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (27074, 50)\n",
      "x_test shape: (6768, 50)\n",
      "Build model...\n",
      "Train on 27074 samples, validate on 6768 samples\n",
      "Epoch 1/35\n",
      "27074/27074 [==============================] - 10s 362us/step - loss: 3.0824 - acc: 0.3694 - val_loss: 2.6357 - val_acc: 0.3791\n",
      "Epoch 2/35\n",
      "27074/27074 [==============================] - 9s 335us/step - loss: 1.8074 - acc: 0.5637 - val_loss: 2.0825 - val_acc: 0.4922\n",
      "Epoch 3/35\n",
      "27074/27074 [==============================] - 9s 336us/step - loss: 1.3038 - acc: 0.6893 - val_loss: 1.8155 - val_acc: 0.5557\n",
      "Epoch 4/35\n",
      "27074/27074 [==============================] - 9s 334us/step - loss: 0.9899 - acc: 0.7679 - val_loss: 1.6808 - val_acc: 0.5836\n",
      "Epoch 5/35\n",
      "27074/27074 [==============================] - 9s 335us/step - loss: 0.7718 - acc: 0.8220 - val_loss: 1.6073 - val_acc: 0.6037\n",
      "Epoch 6/35\n",
      "27074/27074 [==============================] - 9s 334us/step - loss: 0.6159 - acc: 0.8550 - val_loss: 1.5749 - val_acc: 0.6111\n",
      "Epoch 7/35\n",
      "27074/27074 [==============================] - 9s 335us/step - loss: 0.5053 - acc: 0.8766 - val_loss: 1.5573 - val_acc: 0.6150\n",
      "Epoch 8/35\n",
      "27074/27074 [==============================] - 9s 329us/step - loss: 0.4283 - acc: 0.8898 - val_loss: 1.5518 - val_acc: 0.6212\n",
      "Epoch 9/35\n",
      "27074/27074 [==============================] - 9s 330us/step - loss: 0.3751 - acc: 0.8980 - val_loss: 1.5514 - val_acc: 0.6272\n",
      "Epoch 10/35\n",
      "27074/27074 [==============================] - 9s 330us/step - loss: 0.3380 - acc: 0.9025 - val_loss: 1.5585 - val_acc: 0.6303\n",
      "Epoch 11/35\n",
      "27074/27074 [==============================] - 9s 330us/step - loss: 0.3126 - acc: 0.9051 - val_loss: 1.5582 - val_acc: 0.6328\n",
      "Epoch 12/35\n",
      "27074/27074 [==============================] - 9s 330us/step - loss: 0.2943 - acc: 0.9095 - val_loss: 1.5759 - val_acc: 0.6345\n",
      "                                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                                            aprons       1.00      1.00      1.00        44\n",
      "                                                     baking dishes       0.95      0.86      0.90        69\n",
      "                                       baking sheets & baking pans       0.79      0.89      0.84        35\n",
      "                                                          blenders       0.96      0.86      0.91        29\n",
      "                                         bottle stoppers & pourers       1.00      0.77      0.87        22\n",
      "                                                             bowls       0.83      0.86      0.85       389\n",
      "                                   chopping/slicing/cutting boards       0.94      1.00      0.97        61\n",
      "                                                          coasters       0.99      1.00      0.99        68\n",
      "                                        cocktail preparation tools       0.95      1.00      0.98        40\n",
      "                                    coffee, tea, & espresso makers       0.92      0.98      0.95        83\n",
      "                         coffee, tea, & espresso preparation tools       0.57      0.40      0.47        20\n",
      "                                             colanders & strainers       1.00      0.71      0.83        14\n",
      "                                    cooking utensils variety packs       0.79      1.00      0.89        31\n",
      "                                  cooking/baking spoons & spatulas       0.84      0.68      0.75        38\n",
      "               cookware & bakeware replacement parts & accessories       0.70      0.50      0.58        14\n",
      "                                 cookware & bakeware storage racks       0.90      1.00      0.95        26\n",
      "                                 cookware & bakeware variety packs       0.90      0.81      0.85        43\n",
      "                                           coolers & food carriers       0.93      1.00      0.96       222\n",
      "                                                 cruets & sprayers       1.00      1.00      1.00        20\n",
      "                                                         decanters       0.24      0.90      0.39        40\n",
      "                                          dish drying racks & mats       0.92      1.00      0.96        22\n",
      "                                                        dispensers       1.00      0.83      0.91        29\n",
      "                                           disposable food storage       0.19      0.53      0.28        15\n",
      "                                              dough & baking tools       0.91      0.59      0.71        17\n",
      "                                            dutch ovens & braisers       0.96      1.00      0.98        50\n",
      "                                                     flatware sets       0.86      0.95      0.90       119\n",
      "food & beverage preparation device accessories & replacement parts       0.82      0.82      0.82        22\n",
      "                  food decorating tools & presentation accessories       0.73      0.73      0.73        15\n",
      "                                     food preparation/mixing bowls       0.91      0.87      0.89        23\n",
      "                                                   food processors       0.89      0.94      0.91        17\n",
      "                                              food storage baskets       0.83      0.88      0.86        17\n",
      "                   food storage canisters/containers & accessories       0.95      0.78      0.86       306\n",
      "                                                             forks       1.00      0.75      0.86        16\n",
      "                                            frying pans & skillets       0.82      0.87      0.85        70\n",
      "                                                           glasses       0.96      0.85      0.90       867\n",
      "                                             grill pans & griddles       0.85      0.85      0.85        27\n",
      "                                 grinders, juicers, & ice crushers       0.89      0.73      0.80        22\n",
      "                                ice buckets & wine bottle chillers       0.94      0.98      0.96        49\n",
      "                                 kitchen merchandise variety packs       0.75      0.69      0.72        58\n",
      "       kitchen textiles & clothing replacement parts & accessories       1.00      0.96      0.98        77\n",
      "                                   kitchen/dishtowels & dishcloths       1.00      1.00      1.00       158\n",
      "                                      knife blocks, racks, & rolls       1.00      0.12      0.22        16\n",
      "                                                            knives       0.95      0.93      0.94        95\n",
      "                                                    measuring cups       1.00      1.00      1.00        15\n",
      "                                                            mixers       0.98      1.00      0.99        42\n",
      "                                                 mugs/teacups/cups       0.79      0.92      0.85       247\n",
      "                                                           napkins       0.98      0.99      0.99       228\n",
      "                                                           openers       1.00      1.00      1.00        47\n",
      "                                       peelers, slicers, & graters       0.88      0.96      0.92        23\n",
      "                                                pitchers & carafes       1.00      0.93      0.96        56\n",
      "                                                         placemats       1.00      1.00      1.00       214\n",
      "                                                 plates & platters       0.88      0.96      0.92       552\n",
      "                                           potholders & oven mitts       0.98      1.00      0.99        40\n",
      "                                        reusable food storage bags       0.85      0.92      0.88        12\n",
      "                                                         saucepans       0.81      0.81      0.81        26\n",
      "                                         seasoning shakers & mills       0.98      1.00      0.99        44\n",
      "                                                  serving utensils       0.71      0.55      0.62        22\n",
      "                          serving utensils & cutlery variety packs       0.58      0.47      0.52        15\n",
      "                                                    shaker bottles       1.00      0.78      0.88        32\n",
      "                                           slow & pressure cookers       0.88      1.00      0.93        14\n",
      "                                          specialty serving dishes       0.94      0.48      0.64        31\n",
      "                                                       spice racks       1.00      0.73      0.85        15\n",
      "                                                       spoon rests       1.00      1.00      1.00        13\n",
      "                                                            spoons       0.76      0.66      0.70        29\n",
      "                                                            stands       0.84      0.79      0.81        33\n",
      "                                      steamers, pasta & stock pots       0.85      0.65      0.73        17\n",
      "                                       tablecloths & table runners       0.99      1.00      1.00       392\n",
      "                                   tableware storage & accessories       0.85      1.00      0.92        11\n",
      "                                           tableware variety packs       0.80      0.81      0.81       700\n",
      "                                              teapots & teakettles       1.00      0.93      0.97        45\n",
      "                                          toasters & toaster ovens       1.00      0.96      0.98        28\n",
      "                                                    trays & boards       0.86      0.66      0.75       173\n",
      "                                                           trivets       1.00      0.95      0.98        21\n",
      "                                       turntables/lazy susan trays       0.91      0.77      0.83        13\n",
      "                                            utensil holders/crocks       0.77      0.85      0.81        20\n",
      "                                vacuum/water bottles & travel mugs       0.80      0.82      0.81       156\n",
      "                                       wine racks & bottle holders       0.96      1.00      0.98        27\n",
      "\n",
      "                                                          accuracy                           0.88      6768\n",
      "                                                         macro avg       0.88      0.85      0.85      6768\n",
      "                                                      weighted avg       0.90      0.88      0.89      6768\n",
      "\n",
      "in Fold  4\n",
      "27087 train sequences\n",
      "6755 test sequences\n",
      "Average train sequence length: 6.520914091630671\n",
      "Average test sequence length: 6.105107327905255\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (27087, 16)\n",
      "x_test shape: (6755, 16)\n",
      "Build model...\n",
      "Train on 27087 samples, validate on 6755 samples\n",
      "Epoch 1/35\n",
      "27087/27087 [==============================] - 3s 109us/step - loss: 3.2278 - acc: 0.2713 - val_loss: 2.6018 - val_acc: 0.4549\n",
      "Epoch 2/35\n",
      "27087/27087 [==============================] - 3s 96us/step - loss: 1.9433 - acc: 0.6321 - val_loss: 1.5427 - val_acc: 0.7044\n",
      "Epoch 3/35\n",
      "27087/27087 [==============================] - 3s 97us/step - loss: 1.1833 - acc: 0.7653 - val_loss: 1.0722 - val_acc: 0.7858\n",
      "Epoch 4/35\n",
      "27087/27087 [==============================] - 3s 96us/step - loss: 0.8381 - acc: 0.8289 - val_loss: 0.8111 - val_acc: 0.8369\n",
      "Epoch 5/35\n",
      "27087/27087 [==============================] - 3s 100us/step - loss: 0.6262 - acc: 0.8703 - val_loss: 0.6353 - val_acc: 0.8614\n",
      "Epoch 6/35\n",
      "27087/27087 [==============================] - 3s 95us/step - loss: 0.4817 - acc: 0.8983 - val_loss: 0.5163 - val_acc: 0.8838\n",
      "Epoch 7/35\n",
      "27087/27087 [==============================] - 3s 95us/step - loss: 0.3792 - acc: 0.9226 - val_loss: 0.4331 - val_acc: 0.9085\n",
      "Epoch 8/35\n",
      "27087/27087 [==============================] - 3s 95us/step - loss: 0.3043 - acc: 0.9407 - val_loss: 0.3783 - val_acc: 0.9212\n",
      "Epoch 9/35\n",
      "27087/27087 [==============================] - 3s 98us/step - loss: 0.2489 - acc: 0.9510 - val_loss: 0.3370 - val_acc: 0.9298\n",
      "Epoch 10/35\n",
      "27087/27087 [==============================] - 3s 101us/step - loss: 0.2070 - acc: 0.9594 - val_loss: 0.3079 - val_acc: 0.9325\n",
      "Epoch 11/35\n",
      "27087/27087 [==============================] - 3s 101us/step - loss: 0.1753 - acc: 0.9649 - val_loss: 0.2842 - val_acc: 0.9349\n",
      "Epoch 12/35\n",
      "27087/27087 [==============================] - 3s 101us/step - loss: 0.1502 - acc: 0.9693 - val_loss: 0.2686 - val_acc: 0.9372\n",
      "Epoch 13/35\n",
      "27087/27087 [==============================] - 3s 104us/step - loss: 0.1303 - acc: 0.9730 - val_loss: 0.2555 - val_acc: 0.9392\n",
      "Epoch 14/35\n",
      "27087/27087 [==============================] - 3s 96us/step - loss: 0.1142 - acc: 0.9760 - val_loss: 0.2478 - val_acc: 0.9403\n",
      "Epoch 15/35\n",
      "27087/27087 [==============================] - 3s 95us/step - loss: 0.1008 - acc: 0.9785 - val_loss: 0.2399 - val_acc: 0.9400\n",
      "Epoch 16/35\n",
      "27087/27087 [==============================] - 3s 95us/step - loss: 0.0896 - acc: 0.9807 - val_loss: 0.2365 - val_acc: 0.9412\n",
      "Epoch 17/35\n",
      "27087/27087 [==============================] - 3s 95us/step - loss: 0.0802 - acc: 0.9824 - val_loss: 0.2325 - val_acc: 0.9418\n",
      "Epoch 18/35\n",
      "27087/27087 [==============================] - 3s 95us/step - loss: 0.0718 - acc: 0.9840 - val_loss: 0.2309 - val_acc: 0.9418\n",
      "Epoch 19/35\n",
      "27087/27087 [==============================] - 3s 97us/step - loss: 0.0648 - acc: 0.9855 - val_loss: 0.2308 - val_acc: 0.9418\n",
      "Epoch 20/35\n",
      "27087/27087 [==============================] - 3s 96us/step - loss: 0.0586 - acc: 0.9869 - val_loss: 0.2302 - val_acc: 0.9429\n",
      "Epoch 21/35\n",
      "27087/27087 [==============================] - 3s 97us/step - loss: 0.0530 - acc: 0.9876 - val_loss: 0.2312 - val_acc: 0.9432\n",
      "Epoch 22/35\n",
      "27087/27087 [==============================] - 3s 95us/step - loss: 0.0482 - acc: 0.9889 - val_loss: 0.2319 - val_acc: 0.9434\n",
      "Epoch 23/35\n",
      "27087/27087 [==============================] - 3s 95us/step - loss: 0.0438 - acc: 0.9899 - val_loss: 0.2328 - val_acc: 0.9408\n",
      "27087 train sequences\n",
      "6755 test sequences\n",
      "Average train sequence length: 62.24269944992063\n",
      "Average test sequence length: 58.412879348630646\n",
      "Adding 2-gram features\n",
      "max_features: 243766\n",
      "Average train sequence length: 123\n",
      "Average test sequence length: 107\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (27087, 50)\n",
      "x_test shape: (6755, 50)\n",
      "Build model...\n",
      "Train on 27087 samples, validate on 6755 samples\n",
      "Epoch 1/35\n",
      "27087/27087 [==============================] - 10s 361us/step - loss: 3.1236 - acc: 0.3443 - val_loss: 2.4930 - val_acc: 0.3997\n",
      "Epoch 2/35\n",
      "27087/27087 [==============================] - 9s 332us/step - loss: 1.8300 - acc: 0.5613 - val_loss: 1.9776 - val_acc: 0.5221\n",
      "Epoch 3/35\n",
      "27087/27087 [==============================] - 9s 329us/step - loss: 1.3232 - acc: 0.6833 - val_loss: 1.7156 - val_acc: 0.5816\n",
      "Epoch 4/35\n",
      "27087/27087 [==============================] - 9s 333us/step - loss: 1.0070 - acc: 0.7624 - val_loss: 1.5705 - val_acc: 0.6083\n",
      "Epoch 5/35\n",
      "27087/27087 [==============================] - 9s 336us/step - loss: 0.7841 - acc: 0.8157 - val_loss: 1.4856 - val_acc: 0.6132\n",
      "Epoch 6/35\n",
      "27087/27087 [==============================] - 9s 337us/step - loss: 0.6238 - acc: 0.8518 - val_loss: 1.4314 - val_acc: 0.6269\n",
      "Epoch 7/35\n",
      "27087/27087 [==============================] - 9s 336us/step - loss: 0.5107 - acc: 0.8757 - val_loss: 1.4067 - val_acc: 0.6352\n",
      "Epoch 8/35\n",
      "27087/27087 [==============================] - 9s 338us/step - loss: 0.4323 - acc: 0.8893 - val_loss: 1.3972 - val_acc: 0.6400\n",
      "Epoch 9/35\n",
      "27087/27087 [==============================] - 9s 338us/step - loss: 0.3787 - acc: 0.8964 - val_loss: 1.3970 - val_acc: 0.6423\n",
      "Epoch 10/35\n",
      "27087/27087 [==============================] - 9s 337us/step - loss: 0.3421 - acc: 0.9013 - val_loss: 1.3904 - val_acc: 0.6480\n",
      "Epoch 11/35\n",
      "27087/27087 [==============================] - 9s 336us/step - loss: 0.3176 - acc: 0.9037 - val_loss: 1.3962 - val_acc: 0.6500\n",
      "Epoch 12/35\n",
      "27087/27087 [==============================] - 9s 337us/step - loss: 0.2999 - acc: 0.9059 - val_loss: 1.4133 - val_acc: 0.6520\n",
      "Epoch 13/35\n",
      "27087/27087 [==============================] - 9s 337us/step - loss: 0.2876 - acc: 0.9077 - val_loss: 1.4225 - val_acc: 0.6533\n",
      "                                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                                            aprons       0.94      1.00      0.97        44\n",
      "                                                     baking dishes       0.96      0.96      0.96        69\n",
      "                                       baking sheets & baking pans       0.89      0.94      0.92        35\n",
      "                                                          blenders       1.00      0.97      0.98        29\n",
      "                                         bottle stoppers & pourers       0.94      0.77      0.85        22\n",
      "                                                             bowls       0.84      0.93      0.88       388\n",
      "                                   chopping/slicing/cutting boards       0.97      0.97      0.97        61\n",
      "                                                          coasters       1.00      1.00      1.00        68\n",
      "                                        cocktail preparation tools       0.90      0.90      0.90        40\n",
      "                                    coffee, tea, & espresso makers       0.90      0.98      0.94        82\n",
      "                         coffee, tea, & espresso preparation tools       0.71      0.50      0.59        20\n",
      "                                             colanders & strainers       1.00      0.92      0.96        13\n",
      "                                    cooking utensils variety packs       0.46      1.00      0.63        31\n",
      "                                  cooking/baking spoons & spatulas       0.92      0.89      0.91        38\n",
      "               cookware & bakeware replacement parts & accessories       0.81      0.93      0.87        14\n",
      "                                 cookware & bakeware storage racks       0.85      0.88      0.87        26\n",
      "                                 cookware & bakeware variety packs       0.87      0.79      0.83        43\n",
      "                                           coolers & food carriers       0.96      0.96      0.96       221\n",
      "                                                 cruets & sprayers       0.91      1.00      0.95        20\n",
      "                                                         decanters       0.83      0.95      0.88        40\n",
      "                                          dish drying racks & mats       1.00      0.91      0.95        22\n",
      "                                                        dispensers       1.00      1.00      1.00        28\n",
      "                                           disposable food storage       0.85      0.73      0.79        15\n",
      "                                              dough & baking tools       1.00      0.82      0.90        17\n",
      "                                            dutch ovens & braisers       1.00      0.90      0.95        50\n",
      "                                                     flatware sets       0.89      0.95      0.92       119\n",
      "food & beverage preparation device accessories & replacement parts       0.88      0.68      0.77        22\n",
      "                  food decorating tools & presentation accessories       1.00      0.80      0.89        15\n",
      "                                     food preparation/mixing bowls       0.84      0.70      0.76        23\n",
      "                                                   food processors       1.00      0.88      0.93        16\n",
      "                                              food storage baskets       0.83      0.88      0.86        17\n",
      "                   food storage canisters/containers & accessories       0.92      0.88      0.90       306\n",
      "                                                             forks       1.00      0.62      0.77        16\n",
      "                                            frying pans & skillets       0.91      0.96      0.93        70\n",
      "                                                           glasses       0.96      0.96      0.96       867\n",
      "                                             grill pans & griddles       1.00      0.81      0.90        27\n",
      "                                 grinders, juicers, & ice crushers       0.95      0.86      0.90        22\n",
      "                                ice buckets & wine bottle chillers       0.98      0.85      0.91        48\n",
      "                                 kitchen merchandise variety packs       0.58      0.48      0.53        58\n",
      "       kitchen textiles & clothing replacement parts & accessories       0.96      1.00      0.98        76\n",
      "                                   kitchen/dishtowels & dishcloths       0.97      1.00      0.98       158\n",
      "                                      knife blocks, racks, & rolls       0.86      0.38      0.52        16\n",
      "                                                            knives       0.87      0.87      0.87        95\n",
      "                                                    measuring cups       0.94      1.00      0.97        15\n",
      "                                                            mixers       1.00      0.98      0.99        42\n",
      "                                                 mugs/teacups/cups       0.88      0.91      0.90       246\n",
      "                                                           napkins       1.00      1.00      1.00       227\n",
      "                                                           openers       0.94      0.98      0.96        47\n",
      "                                       peelers, slicers, & graters       1.00      0.96      0.98        23\n",
      "                                                pitchers & carafes       0.94      0.91      0.93        56\n",
      "                                                         placemats       1.00      1.00      1.00       214\n",
      "                                                 plates & platters       0.94      0.94      0.94       551\n",
      "                                           potholders & oven mitts       1.00      0.90      0.95        40\n",
      "                                        reusable food storage bags       0.86      0.50      0.63        12\n",
      "                                                         saucepans       0.74      0.92      0.82        25\n",
      "                                         seasoning shakers & mills       0.96      0.98      0.97        44\n",
      "                                                  serving utensils       0.71      0.45      0.56        22\n",
      "                          serving utensils & cutlery variety packs       0.80      0.53      0.64        15\n",
      "                                                    shaker bottles       1.00      1.00      1.00        32\n",
      "                                           slow & pressure cookers       1.00      1.00      1.00        14\n",
      "                                          specialty serving dishes       0.68      0.68      0.68        31\n",
      "                                                       spice racks       0.91      0.67      0.77        15\n",
      "                                                       spoon rests       1.00      1.00      1.00        13\n",
      "                                                            spoons       0.91      0.72      0.81        29\n",
      "                                                            stands       0.94      0.91      0.92        33\n",
      "                                      steamers, pasta & stock pots       0.80      0.71      0.75        17\n",
      "                                       tablecloths & table runners       1.00      1.00      1.00       392\n",
      "                                   tableware storage & accessories       1.00      0.82      0.90        11\n",
      "                                           tableware variety packs       0.94      0.95      0.95       700\n",
      "                                              teapots & teakettles       0.98      0.93      0.95        44\n",
      "                                          toasters & toaster ovens       0.90      1.00      0.95        28\n",
      "                                                    trays & boards       0.94      0.92      0.93       173\n",
      "                                                           trivets       1.00      1.00      1.00        21\n",
      "                                       turntables/lazy susan trays       0.92      0.92      0.92        13\n",
      "                                            utensil holders/crocks       0.90      0.90      0.90        20\n",
      "                                vacuum/water bottles & travel mugs       0.94      0.88      0.91       156\n",
      "                                       wine racks & bottle holders       0.83      0.93      0.88        27\n",
      "\n",
      "                                                          accuracy                           0.93      6755\n",
      "                                                         macro avg       0.91      0.87      0.88      6755\n",
      "                                                      weighted avg       0.93      0.93      0.93      6755\n",
      "\n",
      "in Fold  5\n",
      "27104 train sequences\n",
      "6738 test sequences\n",
      "Average train sequence length: 6.4605593270365995\n",
      "Average test sequence length: 6.346838824577026\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (27104, 16)\n",
      "x_test shape: (6738, 16)\n",
      "Build model...\n",
      "Train on 27104 samples, validate on 6738 samples\n",
      "Epoch 1/35\n",
      "27104/27104 [==============================] - 3s 113us/step - loss: 3.2122 - acc: 0.2784 - val_loss: 2.6150 - val_acc: 0.4283\n",
      "Epoch 2/35\n",
      "27104/27104 [==============================] - 3s 101us/step - loss: 1.8995 - acc: 0.6368 - val_loss: 1.6375 - val_acc: 0.6735\n",
      "Epoch 3/35\n",
      "27104/27104 [==============================] - 3s 97us/step - loss: 1.1464 - acc: 0.7743 - val_loss: 1.1881 - val_acc: 0.7575\n",
      "Epoch 4/35\n",
      "27104/27104 [==============================] - 3s 97us/step - loss: 0.7986 - acc: 0.8354 - val_loss: 0.9571 - val_acc: 0.7965\n",
      "Epoch 5/35\n",
      "27104/27104 [==============================] - 3s 97us/step - loss: 0.5902 - acc: 0.8797 - val_loss: 0.8190 - val_acc: 0.8256\n",
      "Epoch 6/35\n",
      "27104/27104 [==============================] - 3s 96us/step - loss: 0.4518 - acc: 0.9079 - val_loss: 0.7109 - val_acc: 0.8458\n",
      "Epoch 7/35\n",
      "27104/27104 [==============================] - 3s 100us/step - loss: 0.3543 - acc: 0.9297 - val_loss: 0.6400 - val_acc: 0.8657\n",
      "Epoch 8/35\n",
      "27104/27104 [==============================] - 3s 97us/step - loss: 0.2837 - acc: 0.9462 - val_loss: 0.5865 - val_acc: 0.8756\n",
      "Epoch 9/35\n",
      "27104/27104 [==============================] - 3s 97us/step - loss: 0.2319 - acc: 0.9563 - val_loss: 0.5460 - val_acc: 0.8844\n",
      "Epoch 10/35\n",
      "27104/27104 [==============================] - 3s 98us/step - loss: 0.1930 - acc: 0.9623 - val_loss: 0.5250 - val_acc: 0.8850\n",
      "Epoch 11/35\n",
      "27104/27104 [==============================] - 3s 99us/step - loss: 0.1635 - acc: 0.9672 - val_loss: 0.5094 - val_acc: 0.8905\n",
      "Epoch 12/35\n",
      "27104/27104 [==============================] - 3s 96us/step - loss: 0.1401 - acc: 0.9716 - val_loss: 0.5005 - val_acc: 0.8917\n",
      "Epoch 13/35\n",
      "27104/27104 [==============================] - 3s 97us/step - loss: 0.1217 - acc: 0.9750 - val_loss: 0.4837 - val_acc: 0.8955\n",
      "Epoch 14/35\n",
      "27104/27104 [==============================] - 3s 97us/step - loss: 0.1070 - acc: 0.9775 - val_loss: 0.4922 - val_acc: 0.8967\n",
      "Epoch 15/35\n",
      "27104/27104 [==============================] - 3s 97us/step - loss: 0.0945 - acc: 0.9795 - val_loss: 0.4896 - val_acc: 0.8966\n",
      "Epoch 16/35\n",
      "27104/27104 [==============================] - 3s 97us/step - loss: 0.0842 - acc: 0.9816 - val_loss: 0.4951 - val_acc: 0.8979\n",
      "27104 train sequences\n",
      "6738 test sequences\n",
      "Average train sequence length: 61.43137544273908\n",
      "Average test sequence length: 61.66681507865835\n",
      "Adding 2-gram features\n",
      "max_features: 236957\n",
      "Average train sequence length: 121\n",
      "Average test sequence length: 111\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (27104, 50)\n",
      "x_test shape: (6738, 50)\n",
      "Build model...\n",
      "Train on 27104 samples, validate on 6738 samples\n",
      "Epoch 1/35\n",
      "27104/27104 [==============================] - 10s 361us/step - loss: 3.1234 - acc: 0.3453 - val_loss: 2.5469 - val_acc: 0.3896\n",
      "Epoch 2/35\n",
      "27104/27104 [==============================] - 9s 330us/step - loss: 1.7987 - acc: 0.5805 - val_loss: 2.0894 - val_acc: 0.4789\n",
      "Epoch 3/35\n",
      "27104/27104 [==============================] - 9s 329us/step - loss: 1.2889 - acc: 0.6959 - val_loss: 1.8471 - val_acc: 0.5450\n",
      "Epoch 4/35\n",
      "27104/27104 [==============================] - 9s 330us/step - loss: 0.9907 - acc: 0.7625 - val_loss: 1.6944 - val_acc: 0.5827\n",
      "Epoch 5/35\n",
      "27104/27104 [==============================] - 9s 330us/step - loss: 0.7863 - acc: 0.8080 - val_loss: 1.6081 - val_acc: 0.6051\n",
      "Epoch 6/35\n",
      "27104/27104 [==============================] - 9s 329us/step - loss: 0.6378 - acc: 0.8452 - val_loss: 1.5517 - val_acc: 0.6156\n",
      "Epoch 7/35\n",
      "27104/27104 [==============================] - 9s 330us/step - loss: 0.5306 - acc: 0.8671 - val_loss: 1.5262 - val_acc: 0.6253\n",
      "Epoch 8/35\n",
      "27104/27104 [==============================] - 9s 331us/step - loss: 0.4553 - acc: 0.8801 - val_loss: 1.4942 - val_acc: 0.6315\n",
      "Epoch 9/35\n",
      "27104/27104 [==============================] - 9s 331us/step - loss: 0.4034 - acc: 0.8867 - val_loss: 1.4935 - val_acc: 0.6397\n",
      "Epoch 10/35\n",
      "27104/27104 [==============================] - 9s 330us/step - loss: 0.3688 - acc: 0.8903 - val_loss: 1.4682 - val_acc: 0.6480\n",
      "Epoch 11/35\n",
      "27104/27104 [==============================] - 9s 329us/step - loss: 0.3453 - acc: 0.8935 - val_loss: 1.4814 - val_acc: 0.6478\n",
      "Epoch 12/35\n",
      "27104/27104 [==============================] - 9s 329us/step - loss: 0.3284 - acc: 0.8958 - val_loss: 1.4829 - val_acc: 0.6492\n",
      "Epoch 13/35\n",
      "27104/27104 [==============================] - 9s 325us/step - loss: 0.3163 - acc: 0.8967 - val_loss: 1.4916 - val_acc: 0.6511\n",
      "                                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                                            aprons       0.98      0.98      0.98        43\n",
      "                                                     baking dishes       0.69      0.97      0.81        69\n",
      "                                       baking sheets & baking pans       0.77      0.69      0.73        35\n",
      "                                                          blenders       0.88      1.00      0.94        29\n",
      "                                         bottle stoppers & pourers       0.70      0.86      0.78        22\n",
      "                                                             bowls       0.89      0.63      0.74       388\n",
      "                                   chopping/slicing/cutting boards       0.97      1.00      0.98        60\n",
      "                                                          coasters       1.00      0.68      0.81        68\n",
      "                                        cocktail preparation tools       0.97      0.95      0.96        40\n",
      "                                    coffee, tea, & espresso makers       0.93      0.90      0.91        82\n",
      "                         coffee, tea, & espresso preparation tools       0.70      0.80      0.74        20\n",
      "                                             colanders & strainers       0.79      0.85      0.81        13\n",
      "                                    cooking utensils variety packs       0.75      0.58      0.65        31\n",
      "                                  cooking/baking spoons & spatulas       0.79      0.81      0.80        37\n",
      "               cookware & bakeware replacement parts & accessories       0.75      0.64      0.69        14\n",
      "                                 cookware & bakeware storage racks       1.00      0.58      0.73        26\n",
      "                                 cookware & bakeware variety packs       0.80      0.77      0.79        43\n",
      "                                           coolers & food carriers       0.95      0.95      0.95       221\n",
      "                                                 cruets & sprayers       1.00      0.70      0.82        20\n",
      "                                                         decanters       0.53      0.65      0.58        40\n",
      "                                          dish drying racks & mats       0.94      0.81      0.87        21\n",
      "                                                        dispensers       0.96      0.86      0.91        28\n",
      "                                           disposable food storage       0.91      0.67      0.77        15\n",
      "                                              dough & baking tools       1.00      0.71      0.83        17\n",
      "                                            dutch ovens & braisers       0.85      0.82      0.84        50\n",
      "                                                     flatware sets       0.75      0.67      0.71       119\n",
      "food & beverage preparation device accessories & replacement parts       0.90      0.41      0.56        22\n",
      "                  food decorating tools & presentation accessories       1.00      0.36      0.53        14\n",
      "                                     food preparation/mixing bowls       0.81      0.57      0.67        23\n",
      "                                                   food processors       1.00      0.94      0.97        16\n",
      "                                              food storage baskets       0.70      0.82      0.76        17\n",
      "                   food storage canisters/containers & accessories       0.91      0.91      0.91       306\n",
      "                                                             forks       1.00      1.00      1.00        16\n",
      "                                            frying pans & skillets       0.92      0.81      0.86        70\n",
      "                                                           glasses       0.93      0.98      0.95       867\n",
      "                                             grill pans & griddles       0.95      0.74      0.83        27\n",
      "                                 grinders, juicers, & ice crushers       0.86      0.90      0.88        21\n",
      "                                ice buckets & wine bottle chillers       0.89      0.81      0.85        48\n",
      "                                 kitchen merchandise variety packs       0.69      0.58      0.63        57\n",
      "       kitchen textiles & clothing replacement parts & accessories       0.99      0.99      0.99        76\n",
      "                                   kitchen/dishtowels & dishcloths       1.00      0.97      0.98       158\n",
      "                                      knife blocks, racks, & rolls       0.79      0.69      0.73        16\n",
      "                                                            knives       0.84      0.73      0.78        95\n",
      "                                                    measuring cups       0.86      0.86      0.86        14\n",
      "                                                            mixers       0.98      0.98      0.98        41\n",
      "                                                 mugs/teacups/cups       0.98      0.72      0.83       246\n",
      "                                                           napkins       0.99      1.00      1.00       227\n",
      "                                                           openers       0.97      0.80      0.88        46\n",
      "                                       peelers, slicers, & graters       0.80      0.87      0.83        23\n",
      "                                                pitchers & carafes       0.94      0.79      0.85        56\n",
      "                                                         placemats       0.98      1.00      0.99       214\n",
      "                                                 plates & platters       0.97      0.82      0.89       551\n",
      "                                           potholders & oven mitts       0.90      0.95      0.92        39\n",
      "                                        reusable food storage bags       0.67      0.55      0.60        11\n",
      "                                                         saucepans       0.92      0.96      0.94        25\n",
      "                                         seasoning shakers & mills       0.96      0.98      0.97        44\n",
      "                                                  serving utensils       0.64      0.64      0.64        22\n",
      "                          serving utensils & cutlery variety packs       0.29      0.13      0.18        15\n",
      "                                                    shaker bottles       0.67      0.81      0.73        32\n",
      "                                           slow & pressure cookers       1.00      0.92      0.96        13\n",
      "                                          specialty serving dishes       0.85      0.57      0.68        30\n",
      "                                                       spice racks       0.62      1.00      0.77        15\n",
      "                                                       spoon rests       1.00      0.92      0.96        13\n",
      "                                                            spoons       0.93      0.97      0.95        29\n",
      "                                                            stands       1.00      0.79      0.88        33\n",
      "                                      steamers, pasta & stock pots       0.92      0.69      0.79        16\n",
      "                                       tablecloths & table runners       0.99      1.00      1.00       391\n",
      "                                   tableware storage & accessories       1.00      0.45      0.62        11\n",
      "                                           tableware variety packs       0.67      1.00      0.80       700\n",
      "                                              teapots & teakettles       0.88      0.98      0.92        44\n",
      "                                          toasters & toaster ovens       1.00      0.86      0.92        28\n",
      "                                                    trays & boards       0.89      0.84      0.86       173\n",
      "                                                           trivets       0.95      1.00      0.98        21\n",
      "                                       turntables/lazy susan trays       0.92      0.92      0.92        12\n",
      "                                            utensil holders/crocks       0.89      0.80      0.84        20\n",
      "                                vacuum/water bottles & travel mugs       0.84      0.89      0.86       156\n",
      "                                       wine racks & bottle holders       0.96      0.85      0.90        27\n",
      "\n",
      "                                                          accuracy                           0.88      6738\n",
      "                                                         macro avg       0.87      0.81      0.83      6738\n",
      "                                                      weighted avg       0.89      0.88      0.87      6738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "\n",
    "score = []\n",
    "\n",
    "X = np.array(name_token)\n",
    "X_des = np.array(des_token)\n",
    "y = np.array(label)\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(X, y)\n",
    "\n",
    "\n",
    "fold = 1\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "    print(\"in Fold \", fold)\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    name_yhat = eval_model(x_train, x_test, y_train, y_test,maxlen=16,fold_number=fold,ngram_range=1,feature='name')\n",
    "\n",
    "    \n",
    "    x_train, x_test = X_des[train_index], X_des[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    des_yhat = eval_model(x_train, x_test, y_train, y_test,maxlen=50,fold_number=fold,ngram_range=2,feature='description')\n",
    "\n",
    "    yhat = np.argmax(name_yhat+des_yhat,axis=1)\n",
    "    \n",
    "    \n",
    "    score.append(f1_score(y_test,yhat,average='macro'))\n",
    "    print(classification_report(y_test, yhat, target_names=CLASSES.classes_))\n",
    "    fold+=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.style.use('ggplot')\n",
    "length=range(1,len(score)+1)\n",
    "   \n",
    "plt.plot(length,score,'s-')\n",
    "\n",
    "# plt.title(\"LSTM\")\n",
    "# plt.xlabel(\"fold number\")\n",
    "# plt.ylabel(\"F1_score\")\n",
    "# plt.legend(loc = \"best\")\n",
    "\n",
    "plt.ylim(0.55, 1)\n",
    "plt.xlim(0, len(score)+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.817686239040642"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7986102128597286,\n",
       " 0.8470952634437343,\n",
       " 0.847074948864232,\n",
       " 0.8414410504561404,\n",
       " 0.7542097195793748]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set parameters:\n",
    "# # ngram_range = 2 will add bi-grams features\n",
    "# ngram_range = 2\n",
    "# max_features = 20000\n",
    "# maxlen = 16\n",
    "# batch_size = 32\n",
    "# embedding_dims = 40\n",
    "# epochs = 35\n",
    "# Patience = 3\n",
    "\n",
    "# print('Loading data...')\n",
    "\n",
    "\n",
    "\n",
    "# # # call load_data with allow_pickle implicitly set to true\n",
    "# # x_train, x_test, y_train, y_test =  train_test_split(sequences, label,\n",
    "# #                                                     stratify=label, \n",
    "# #                                                     test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# # print(len(x_train), 'train sequences')\n",
    "# # print(len(x_test), 'test sequences')\n",
    "# # print('Average train sequence length: {}'.format(\n",
    "# #     np.mean(list(map(len, x_train)))))\n",
    "# # print('Average test sequence length: {}'.format(\n",
    "# #     np.mean(list(map(len, x_test)))))\n",
    "\n",
    "# # if ngram_range > 1:\n",
    "# #     print('Adding {}-gram features'.format(ngram_range))\n",
    "# #     # Create set of unique n-gram from the training set.\n",
    "# #     ngram_set = set()\n",
    "# #     for input_list in x_train:\n",
    "# #         for i in range(2, ngram_range + 1):\n",
    "# #             set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "# #             ngram_set.update(set_of_ngram)\n",
    "\n",
    "# #     # Dictionary mapping n-gram token to a unique integer.\n",
    "# #     # Integer values are greater than max_features in order\n",
    "# #     # to avoid collision with existing features.\n",
    "# #     start_index = max_features + 1\n",
    "# #     token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "# #     indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "# #     # max_features is the highest integer that could be found in the dataset.\n",
    "# #     max_features = np.max(list(indice_token.keys())) + 1\n",
    "# #     print(\"max_features:\",max_features)\n",
    "\n",
    "# #     # Augmenting x_train and x_test with n-grams features\n",
    "# #     x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "# #     x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "# #     print('Average train sequence length: {}'.format(\n",
    "# #         np.mean(list(map(len, x_train)), dtype=int)))\n",
    "# #     print('Average test sequence length: {}'.format(\n",
    "# #         np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "# # print('Pad sequences (samples x time)')\n",
    "# # x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "# # x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "# # print('x_train shape:', x_train.shape)\n",
    "# # print('x_test shape:', x_test.shape)\n",
    "\n",
    "# print('Build model...')\n",
    "# model = Sequential()\n",
    "\n",
    "# # we start off with an efficient embedding layer which maps\n",
    "# # our vocab indices into embedding_dims dimensions\n",
    "# model.add(Embedding(max_features,\n",
    "#                     embedding_dims,\n",
    "#                     input_length=maxlen))\n",
    "\n",
    "# # we add a GlobalAveragePooling1D, which will average the embeddings\n",
    "# # of all words in the document\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "# model.add(Dense(NUM_OF_CLASS, activation='softmax'))\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# print(model.summary())\n",
    "# # checkpoint = ModelCheckpoint(\"best.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "# # history = model.fit(x_train, y_train,\n",
    "# #           batch_size=batch_size,\n",
    "# #           epochs=epochs,\n",
    "# #           validation_data=(x_test, y_test),\n",
    "# #           callbacks=[EarlyStopping(monitor='val_loss', patience=Patience),checkpoint], \n",
    "# #           verbose=1 )\n",
    "\n",
    "# # plotresult(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('best.h5')\n",
    "# yhat = np.argmax(model.predict(x_test),axis=1)\n",
    "# f1_score(y_test,yhat,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf115",
   "language": "python",
   "name": "tf115"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
